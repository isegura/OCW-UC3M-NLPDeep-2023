{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOcwTzrwOCxcJPiSlSSj974"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a7f3a65dbacf41398fef466b9004a55c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ee529dfb897a425ea5d8727e9c404a50","IPY_MODEL_f3eac943e67d45dbaf94d02688b8c630","IPY_MODEL_63ce1bf9f28142978e04ca68ea3f0611"],"layout":"IPY_MODEL_a2d31ab99c76498d9a61bad332f29cd1"}},"ee529dfb897a425ea5d8727e9c404a50":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc5ccfe0668f4fca9b81c896c6e9c37b","placeholder":"​","style":"IPY_MODEL_4092abbe2f1147989957c41b402f2989","value":"  0%"}},"f3eac943e67d45dbaf94d02688b8c630":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_f238cf395066484ea0a8cc21ed024c86","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_633c54387f764eb5a2ced4145c35bc54","value":0}},"63ce1bf9f28142978e04ca68ea3f0611":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b41b09fb0d2640f3a591539c565a4cf6","placeholder":"​","style":"IPY_MODEL_582c41c6864a4850a0e6f3c4c6b8b558","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"a2d31ab99c76498d9a61bad332f29cd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc5ccfe0668f4fca9b81c896c6e9c37b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4092abbe2f1147989957c41b402f2989":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f238cf395066484ea0a8cc21ed024c86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"633c54387f764eb5a2ced4145c35bc54":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b41b09fb0d2640f3a591539c565a4cf6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"582c41c6864a4850a0e6f3c4c6b8b558":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf11c860008945de8e3b5b30da9b2c66":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_92c87187acca4b46998dec4f2a9887f9","IPY_MODEL_f1e987eafd7d430fbc75fb18bbe3143a","IPY_MODEL_29a3c79bb66e4d39af4307db167a061d"],"layout":"IPY_MODEL_4703f2c89f444380a352db8bcbceada2"}},"92c87187acca4b46998dec4f2a9887f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98be03af2e53478da3893707a7ae96c0","placeholder":"​","style":"IPY_MODEL_905a5c6e79d94338b316d6e846a1484c","value":"  0%"}},"f1e987eafd7d430fbc75fb18bbe3143a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1fbcfa0e87c41a3988ea9718c849f37","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_478314d33cb640268186c30cc1a79d1e","value":0}},"29a3c79bb66e4d39af4307db167a061d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a23eeb4e7e174d6cae5b212d4f717030","placeholder":"​","style":"IPY_MODEL_049142f738854fdaa08ed977d4e9f17c","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"4703f2c89f444380a352db8bcbceada2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98be03af2e53478da3893707a7ae96c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"905a5c6e79d94338b316d6e846a1484c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1fbcfa0e87c41a3988ea9718c849f37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"478314d33cb640268186c30cc1a79d1e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a23eeb4e7e174d6cae5b212d4f717030":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"049142f738854fdaa08ed977d4e9f17c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["# T5 for Text summarization (tensorflow)\n","\n","Text summarization is one of the most important NLP applications. This is a very difficult tasks that poses several challenges such as identifying the important content and generate a summary.\n","\n","In this notebook, we will fine-tune the pre-trained T5 for the task of text summarization. T5 has a encoder-decoder architecture. We will use the XSum dataset from Hugging Face Datasets.\n","\n","**The model will be fine-tuned tensorflow framework**."],"metadata":{"id":"97ZiGntwTQLF"}},{"cell_type":"code","execution_count":19,"metadata":{"id":"NcXBQ6WqTKxs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668087452592,"user_tz":-60,"elapsed":3707,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}},"outputId":"1a316e2e-6542-4f9a-856f-df273d154227"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.6.1)\n","Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (0.1.2)\n","Requirement already satisfied: keras_nlp in /usr/local/lib/python3.7/dist-packages (0.3.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.10.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.1.0)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.3.0)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.7)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras_nlp) (2.9.2)\n","Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (from keras_nlp) (2.9.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (1.2.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (7.1.2)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.6)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (1.6.3)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (2.1.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (0.4.0)\n","Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (2.9.0)\n","Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (1.12)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (0.27.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (3.3.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (1.14.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (57.4.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (1.1.2)\n","Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (2.9.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (14.0.6)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (1.50.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (3.1.0)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (3.17.3)\n","Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (2.9.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras_nlp) (0.38.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras_nlp) (1.5.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (1.8.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (3.4.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (0.6.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (0.4.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (1.35.0)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (1.0.1)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (1.3.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (3.2.2)\n","Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->keras_nlp) (0.12.0)\n"]}],"source":["!pip install transformers datasets rouge-score keras_nlp"]},{"cell_type":"markdown","source":["To ignore warning, please run the following cell:"],"metadata":{"id":"2gY4H_S9FLHs"}},{"cell_type":"code","source":["import transformers\n","print(transformers.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2KSAzg2GDV2-","executionInfo":{"status":"ok","timestamp":1668087452592,"user_tz":-60,"elapsed":6,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}},"outputId":"4aec38eb-5949-44ac-e07a-0b4ac943a3d6"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["4.24.0\n"]}]},{"cell_type":"markdown","source":["If you want that warnings are not printed, please run this cell:"],"metadata":{"id":"PZr7hPCjidoj"}},{"cell_type":"code","source":["import os\n","os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"],"metadata":{"id":"bdFWgjzaDgA2","executionInfo":{"status":"ok","timestamp":1668087452592,"user_tz":-60,"elapsed":4,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["## Data\n","we use the dataset xsum that consists of 226,711 news BBC articles accompanied with a one-sentence summary. The articles covers a wide variety of domains (e.g., News, Politics, Sports, Weather, Business, Technology, Science, Health, Family, Education, Entertainment and Arts). \n","\n","The official random split contains 204,045 (90%), 11,332 (5%) and 11,334 (5) documents in training, validation and test sets, respectively.\n","\n","As the dataset is very large, we will use a smaller sample to run this notebook during the class:"],"metadata":{"id":"vrZ-iZkDUYXG"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","REDUCE_DATA = True\n","\n","if REDUCE_DATA:\n","    # we only load a smaller sample of the dataset for training a summarizer during this class \n","    dataset = load_dataset(\"xsum\", split='train[:1%]').shuffle(seed=42)\n","\n","    # As we only got a smaller sample from the traing split, we need to create the splits\n","    dataset = dataset.train_test_split(test_size=0.2, shuffle=False)\n","\n","    SIZE_TEST= 10   #number of examples for test\n","    dataset[\"validation\"] = dataset[\"test\"].select(range(SIZE_TEST,dataset[\"test\"].num_rows))\n","    # we only get SIZE_TEST for test\n","    dataset[\"test\"] = dataset[\"test\"].select(range(SIZE_TEST))\n","else:\n","    # this loads the full dataset; in this case, we don't have to create the splits, because it already contains them. \n","    dataset = load_dataset(\"xsum\")\n","\n","dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sf7zOv9PULB0","executionInfo":{"status":"ok","timestamp":1668087454688,"user_tz":-60,"elapsed":2099,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}},"outputId":"5aa95df7-0606-4261-c551-d60b276cb9ec"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Found cached dataset xsum (/root/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934)\n","WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934/cache-4e8935f7a3554cf3.arrow\n"]},{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['document', 'summary', 'id'],\n","        num_rows: 1632\n","    })\n","    test: Dataset({\n","        features: ['document', 'summary', 'id'],\n","        num_rows: 10\n","    })\n","    validation: Dataset({\n","        features: ['document', 'summary', 'id'],\n","        num_rows: 398\n","    })\n","})"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["We show some instances. We should always obtain the same ids if we set the seed to 42"],"metadata":{"id":"W8kpIStI_diC"}},{"cell_type":"code","source":["print(dataset['train'][0]['id'])  #36884862     (if the dataset was reduced)\n","print(dataset['validation'][0]['id']) #27929646 (if the dataset was reduced)\n","print(dataset['test'][0]['id']) # 34493630 (if the dataset was reduced)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OLBEIwbC_ejx","executionInfo":{"status":"ok","timestamp":1668087454689,"user_tz":-60,"elapsed":5,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}},"outputId":"02e89524-3c2a-46b9-b169-f2cef666ba59"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["36884862\n","36219003\n","34493630\n"]}]},{"cell_type":"markdown","source":["### Tokenization"],"metadata":{"id":"4ZID6Tv7VY51"}},{"cell_type":"code","source":["PREFIX='summarize: '\n","MAX_INPUT_LENGTH = 1043  #  Maximum length of the input to the model. Use 1024 when Transformers v5.\n","MAX_TARGET_LENGTH = 128  # Maximum length of the output by the model"],"metadata":{"id":"ESuGReB6V6BJ","executionInfo":{"status":"ok","timestamp":1668087454689,"user_tz":-60,"elapsed":3,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","model_name = 't5-small'\n","# we must instanciate the tokenizer using model_max_length to increase the maximu length of the model from 512 to \n","tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=MAX_INPUT_LENGTH)\n","# print(tokenizer.model_max_length)\n","\n","def tokenize(examples):\n","    \"\"\"For each example in the dataset examples, the function will tokenize the input document \n","    but also the expected output, that is, its summary. This will be saved into a new field of the dataset with \n","    the name 'labels'. We only need to save the input_ids of the summary.\"\"\"\n","    inputs = [PREFIX + doc for doc in examples[\"document\"]]\n","    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n","\n","    # Setup the tokenizer for targets\n","    # with tokenizer.as_target_tokenizer():\n","    labels = tokenizer(text_target=examples[\"summary\"], max_length=MAX_TARGET_LENGTH, truncation=True)\n","\n","    # we add a new feature labels to contain the encoded output\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","\n","    return model_inputs\n","\n","# we apply the function to the dataset for encoding it\n","encoded_datasets = dataset.map(tokenize, batched=True)\n","encoded_datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":422,"referenced_widgets":["a7f3a65dbacf41398fef466b9004a55c","ee529dfb897a425ea5d8727e9c404a50","f3eac943e67d45dbaf94d02688b8c630","63ce1bf9f28142978e04ca68ea3f0611","a2d31ab99c76498d9a61bad332f29cd1","cc5ccfe0668f4fca9b81c896c6e9c37b","4092abbe2f1147989957c41b402f2989","f238cf395066484ea0a8cc21ed024c86","633c54387f764eb5a2ced4145c35bc54","b41b09fb0d2640f3a591539c565a4cf6","582c41c6864a4850a0e6f3c4c6b8b558","cf11c860008945de8e3b5b30da9b2c66","92c87187acca4b46998dec4f2a9887f9","f1e987eafd7d430fbc75fb18bbe3143a","29a3c79bb66e4d39af4307db167a061d","4703f2c89f444380a352db8bcbceada2","98be03af2e53478da3893707a7ae96c0","905a5c6e79d94338b316d6e846a1484c","c1fbcfa0e87c41a3988ea9718c849f37","478314d33cb640268186c30cc1a79d1e","a23eeb4e7e174d6cae5b212d4f717030","049142f738854fdaa08ed977d4e9f17c"]},"id":"Hm09FkNKVa_8","executionInfo":{"status":"ok","timestamp":1668087456418,"user_tz":-60,"elapsed":1732,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}},"outputId":"e73d4a93-3435-429c-e28b-e39170c81841"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934/cache-781ade25c3ff89a7.arrow\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7f3a65dbacf41398fef466b9004a55c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf11c860008945de8e3b5b30da9b2c66"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['document', 'summary', 'id', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 1632\n","    })\n","    test: Dataset({\n","        features: ['document', 'summary', 'id', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 10\n","    })\n","    validation: Dataset({\n","        features: ['document', 'summary', 'id', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 398\n","    })\n","})"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["## Model\n","\n","We load the model. \n","We also have to define a data collator to pass the input data to the model. By the default the data collator y datacollatorpadding, which is used for text classification. This datacollator is not useful for seq2seq."],"metadata":{"id":"xxU7snRPYKCQ"}},{"cell_type":"code","source":["from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n","model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"khWlOkf0YLpC","executionInfo":{"status":"ok","timestamp":1668087457926,"user_tz":-60,"elapsed":1510,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}},"outputId":"6770db58-3655-4bb6-c893-08f1018aa91d"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n","\n","All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"]}]},{"cell_type":"markdown","source":["We prepare the dataset to be passed to the model:"],"metadata":{"id":"ZFEuZnQqkwDb"}},{"cell_type":"code","source":["BATCH_SIZE= 16\n","\n","train_dataset = encoded_datasets[\"train\"].to_tf_dataset(\n","    batch_size=BATCH_SIZE,\n","    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n","    shuffle=True,\n","    collate_fn=data_collator,\n",")\n","\n","validation_dataset = encoded_datasets[\"validation\"].to_tf_dataset(\n","    batch_size=BATCH_SIZE,\n","    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n","    shuffle=False,\n","    collate_fn=data_collator,\n",")\n","\n","test_dataset = encoded_datasets[\"test\"].to_tf_dataset(\n","    batch_size=BATCH_SIZE,\n","    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n","    shuffle=False,\n","    collate_fn=data_collator,\n",")\n","\n","print('datasets are ready!!!')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GSFyDgpsYii8","executionInfo":{"status":"ok","timestamp":1668087459842,"user_tz":-60,"elapsed":1918,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}},"outputId":"b028239d-c56e-41e9-a049-4b1d58b42831"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["datasets are ready!!!\n"]}]},{"cell_type":"markdown","source":["### Training the model"],"metadata":{"id":"6f19ohFsYwxz"}},{"cell_type":"code","source":["import keras\n","\n","from keras import optimizers\n","LEARNING_RATE = 2e-5  # Learning-rate for training our model\n","\n","optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n","model.compile(optimizer=optimizer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63NjXezhYy0f","executionInfo":{"status":"ok","timestamp":1668087459843,"user_tz":-60,"elapsed":6,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}},"outputId":"d7c16491-e3bf-4428-9f6c-3d344bb9ad2f"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"]}]},{"cell_type":"code","source":["import keras_nlp\n","rouge_L = keras_nlp.metrics.RougeL()\n"],"metadata":{"id":"1e3efjfIaP0N","executionInfo":{"status":"ok","timestamp":1668087459843,"user_tz":-60,"elapsed":4,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["def compute_metric(eval_predictions):\n","    #the predictions and the corresponding reference labels\n","    predictions, labels = eval_predictions\n","\n","    # we have to decode the predictions\n","    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    \n","    # we also have to decode the reference labels\n","    # first, we replace those labels <0 with the token id for padding\n","    for label in labels:\n","        label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n","    # we now decode the reference labels\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    # we calculate rouge_L comparing the decoded labels and the decoded prediction\n","    result = rouge_L(decoded_labels, decoded_predictions)\n","    # We will print only the F1 score, you can use other aggregation metrics as well\n","    result = {\"RougeL\": result[\"f1_score\"]}\n","\n","    # return metric.compute(decoded_labels, decoded_predictions)\n","    return result\n","    "],"metadata":{"id":"R2oIFvF8Y4qY","executionInfo":{"status":"ok","timestamp":1668087459843,"user_tz":-60,"elapsed":4,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["Finally, we can train the model. We define a callback that will compute the metric rouge-L after each epoch. The results will be calculated on the validation_dataset dataset.\n","\n","For our running this notebook during our session class, we will only use three epochs. However, we recommend training the model with all the dataset and with at least 5 epochs (tought you may need to run it using Google Colab Pro!!!). "],"metadata":{"id":"tFcyT58nFmKY"}},{"cell_type":"code","source":["from transformers.keras_callbacks import KerasMetricCallback\n","\n","metric_callback = KerasMetricCallback(\n","    compute_metric, eval_dataset=validation_dataset, \n","    predict_with_generate=True, label_cols=['labels'])\n","\n","MAX_EPOCHS = 3 # we recommend at least 5 epochs\n","\n","model.fit(train_dataset, validation_data=validation_dataset, epochs=MAX_EPOCHS, callbacks=[metric_callback])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XVxiIhVQaZzi","outputId":"10d587a4-d59e-4e16-e45b-aef86c045073","executionInfo":{"status":"ok","timestamp":1668087713648,"user_tz":-60,"elapsed":253808,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}}},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","102/102 [==============================] - ETA: 0s - loss: 3.3702"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/generation_tf_utils.py:1699: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  UserWarning,\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r102/102 [==============================] - 98s 806ms/step - loss: 3.3702 - val_loss: 2.8966 - RougeL: 0.1460\n","Epoch 2/3\n","102/102 [==============================] - 78s 772ms/step - loss: 3.0387 - val_loss: 2.7760 - RougeL: 0.1502\n","Epoch 3/3\n","102/102 [==============================] - 79s 775ms/step - loss: 2.9489 - val_loss: 2.7233 - RougeL: 0.1545\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fe4c3a2d9d0>"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["## Inference\n","You can directly use the model to generate the summary for some text from the test dataset (or any another text). To do this, we create a pipeline object containing the model and the tokenizer."],"metadata":{"id":"Nat_ixkemV-c"}},{"cell_type":"code","source":["from transformers import pipeline\n","MIN_TARGET_LENGTH = 5\n","summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")\n","\n","summarizer(\n","    dataset[\"test\"][0][\"document\"],\n","    min_length=MIN_TARGET_LENGTH,\n","    max_length=MAX_TARGET_LENGTH,\n","    # max_new_tokens=MAX_TARGET_LENGTH,\n",")"],"metadata":{"id":"k28BiRMfmXcN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668087743398,"user_tz":-60,"elapsed":29752,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}},"outputId":"c28ce1bf-d693-4f08-8940-728ef1f1b2f1"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'summary_text': 'Leicester drew 2-2 with Leicester in the Premier League on saturday in the second half of the season.'}]"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["dataset[\"test\"][0][\"summary\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"d-H_uZD1xV-C","executionInfo":{"status":"ok","timestamp":1668087743399,"user_tz":-60,"elapsed":17,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}},"outputId":"fd1a6841-3f69-48a2-c14c-aef880ea1402"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Premier League top scorer Jamie Vardy scored twice as Leicester came from 2-0 down to draw at Southampton.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","source":["## Evaluation\n","We also want to provide some final scores about our model on the test dataset. First, we use the pipeline to generate a summary for each text in the test dataset. "],"metadata":{"id":"VJx8gx_yvwLy"}},{"cell_type":"code","source":["generated_summaries =summarizer(dataset[\"test\"][\"document\"], truncation=True, min_length=MIN_TARGET_LENGTH, max_length=MAX_TARGET_LENGTH)\n","generated_summaries"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CkD0eO6aOUrR","executionInfo":{"status":"ok","timestamp":1668088035461,"user_tz":-60,"elapsed":292071,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}},"outputId":"d3b1ceab-aff9-4ae6-cdbe-f1749bfa384d"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stderr","text":["Your max_length is set to 128, but you input_length is only 91. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n"]}]},{"cell_type":"code","source":["# we save into a list\n","generated_summaries=[example['summary_text'] for example in generated_summaries]\n","# we calculate the rouge L metrics:\n","result = rouge_L(dataset[\"test\"][\"summary\"], generated_summaries)"],"metadata":{"id":"CfkPHuhIiQJA","executionInfo":{"status":"ok","timestamp":1668088035461,"user_tz":-60,"elapsed":13,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["We print the final results:"],"metadata":{"id":"YihC3poex_Ch"}},{"cell_type":"code","source":["import tensorflow as tf\n","#print(\"rouge-L:\", result['precision'], result['recall'], result['f1_score'])\n","print(\"rouge-L -  Precision:\", tf.get_static_value(result['precision']), \", Recal: \", tf.get_static_value(result['recall']), \", f1-score:\", tf.get_static_value(result['f1_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8y4SZfD5yAUE","executionInfo":{"status":"ok","timestamp":1668088045708,"user_tz":-60,"elapsed":305,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"}},"outputId":"104ad3e6-d62f-4e01-c3da-ccdb96ac1877"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["rouge-L -  Precision: 0.19914204 , Recal:  0.1299356 , f1-score: 0.15438351\n"]}]}]}
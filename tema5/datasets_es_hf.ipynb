{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAV7hBMZ3bIB"
      },
      "source": [
        "# Trabajar con Datasets de HuggingFace\n",
        "\n",
        " HuggingFace incluye una gran colección de datasets para una alta variedad de tareas de PLN. También incluye una librería, Datasets, que nos va a facilitar trabajar con  datasets. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMm-ZCT32rgM"
      },
      "source": [
        "Lo primero que vamos a aprender es a cargar y trabajar con un dataset de HuggingFace. \n",
        "\n",
        "El primer paso será instalar la librería Datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1A__GPoP3h11"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1y2HuDB4dvl"
      },
      "source": [
        "Hugginface incluye más de 10,000 datasets para distintas tareas de NLP (https://huggingface.co/datasets). En este notebook, trabajaremos con el dataset  **rotten_tomatoes**, formado por 5.331 reseñas positivas  y 5.331 negativas sobre películas de Rotten Tomatoes.\n",
        "\n",
        "Te animamos a que pruebes con cualquier otro dataset disponible en HuggingFace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OissruTyHAjo"
      },
      "source": [
        "## Obtener información sobre un dataset \n",
        "\n",
        "Sin necesidad de descargar el dataset, es posible obtener información sobre el dataset. \n",
        "Para ello, debemos utilizar la función **load_dataset_builder()**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1jJsYGl2qvF"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset_builder\n",
        "ds_builder = load_dataset_builder(\"rotten_tomatoes\")\n",
        "\n",
        "# Show dataset description\n",
        "print(\"Descripción del dataset:\", ds_builder.info.description)\n",
        "print(\"Características (features) del dataset:\", ds_builder.info.features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3gsHuMs5hV9"
      },
      "source": [
        "Así, hemos podido ver que el dataset está formado por registros con dos campos: el texto, y su label (positiva o negativa). \n",
        "\n",
        "Algunos datasets de huggingface ya contienen splits para training y testing, incluso para validación. Para conocer los splits de un corpus deberemos utilizar la función **get_dataset_split_names** que nos devolver la lista de los nombres de los subconjuntos del dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVYG8VVVBJCf"
      },
      "outputs": [],
      "source": [
        "from datasets import get_dataset_split_names\n",
        "get_dataset_split_names(\"rotten_tomatoes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rK3IbshFsAC"
      },
      "source": [
        "Además de los splits, algunos datasets también incluyen distintas configuraciones. Por ejemplo, el dataset **MInDS-14** está formado por distintos subdatasets, cada uno de ellos para un idioma diferente. \n",
        "Los datos son audio. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fboll_QKFtYE"
      },
      "outputs": [],
      "source": [
        "from datasets import get_dataset_config_names\n",
        "get_dataset_config_names(\"PolyAI/minds14\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7t7pCxzGFnF"
      },
      "source": [
        "Si el dataset no tiene distintas configuraciones, dicha función devolverá el valor 'default' o el nombre del dataset: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk3HUMlSGCfR"
      },
      "outputs": [],
      "source": [
        "get_dataset_config_names(\"rotten_tomatoes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2985O1T57A6"
      },
      "source": [
        "### Ejericio 1:\n",
        "\n",
        "Consulta la información asociada con el dataset **yelp_review_full**\n",
        "- ¿Cuál es el objetivo del dataset?\n",
        "- ¿Cuáles son sus principales características?.\n",
        "- ¿Es distribuido con splits?.\n",
        "- ¿Tiene distintas configuraciones?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhwMZaqa6KeR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ArzlpXEByuQ"
      },
      "source": [
        "## Cargar un dataset \n",
        "\n",
        "Ahora sí vamos a cargar el dataset desde HuggingFace. Para descargar el dataset, usaremos la función **load_dataset**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBRbrLQ9B2w7"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "load_dataset(\"rotten_tomatoes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función anterior no devuelve un dataset, sino un objeto DatasetDict, es decir, un diccionario de datasets. Las claves del diccionario son los splits que contiene el dataset. En este caso: train, validation, y test. Si el dataset no tuviera splits, normalmente el objeto diccionario contiene un único dataset con clave 'train'. \n",
        "Cada elemento del diccionario se corresponde con un dataset (asociado a cada split).\n",
        "\n",
        "La función **load_dataset** también nos permite cargar directamente un split, por ejemplo, en la siguiente celda, se cargará únicamente el split para training:"
      ],
      "metadata": {
        "id": "C9UeHCHn8Syg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzv3kqghCViB"
      },
      "outputs": [],
      "source": [
        "load_dataset(\"rotten_tomatoes\", split=\"train\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lIAkp4DC5oY"
      },
      "source": [
        "Date cuenta que la función ya no devuelve un diccionario, sino un objeto dataset. \n",
        "También es posible cargar distintas combinaciones. Por ejemplo, podemos cargar en un mismo dataset el split de training y validación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZ9MDKoGC9OI"
      },
      "outputs": [],
      "source": [
        "load_dataset(\"rotten_tomatoes\", split=\"train+validation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8ZLC_QsCqUI"
      },
      "source": [
        "A veces incluso no queremos cargar todo el dataset, sino una pequeña muestra. Por ejemplo, en la siguiente celda vamos a recuperar una muestra del split de training, cuyos índices van desde 55 hasta el 59:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvTasa80CsWx"
      },
      "outputs": [],
      "source": [
        "sample = load_dataset('rotten_tomatoes', split='train[55:59]')\n",
        "\n",
        "for i in range(len(sample)):\n",
        "    print(\"Instance: \", i + 55)\n",
        "    print(\"text: \", sample[i]['text'])\n",
        "    print(\"label: \", sample[i]['label'])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otro ejemplo, es cargar el primer 1% del training:"
      ],
      "metadata": {
        "id": "fWoKrBewFHQf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeL_ntQeEh6s"
      },
      "outputs": [],
      "source": [
        "# The first 1% of `train` split.\n",
        "sample = load_dataset('rotten_tomatoes', split='train[:1%]')\n",
        "for i in range(len(sample)):\n",
        "    print(\"Instance: \", i)\n",
        "    print(\"text: \", sample[i]['text'])\n",
        "    print(\"label: \", sample[i]['label'])\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Puedes encontrar más ejemplos en \n",
        "https://huggingface.co/docs/datasets/v1.11.0/splits.html\n"
      ],
      "metadata": {
        "id": "bZzzoY2uEQUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 2: \n",
        "\n",
        "Supongamos que queremos tomar 1/3 del test del dataset **rotten_tomatoes**. En concreto, queremos tomar la tercera parte que está a continuación del primer tercio de ese split. \n",
        "\n",
        "¿Cuál es su primer texto y su label?\n",
        "¿y el último?"
      ],
      "metadata": {
        "id": "ouGQtOQzF6xE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mT0Cqm2VGmv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY3-x-DpGqgP"
      },
      "source": [
        "### Loading a configuration\n",
        "Si el dataset contiene varias configuraciones (por ejemplo, MInDS-14 dataset tiene un total de 14 subconjuntos que dependen del idioma), es posible cargar una configuración específica. Por ejemplo, en la siguiente celda, se cargar el 5% del training del subconjunto en Francés: \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bdsw3kX2HVWg"
      },
      "outputs": [],
      "source": [
        "load_dataset(\"PolyAI/minds14\", \"fr-FR\", split=\"train[:5%]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYUbc6SuS_L_"
      },
      "source": [
        "## Cargar un dataset desde local o remoto\n",
        "\n",
        "Además de cargar datasets de HuggingFace, también va a ser posible cargar datasets desde local o desde algún servidor. \n",
        "\n",
        "Por ejemplo, vamos a cargar el dataset para la tarea de detección de sacarmo, con el que ya hemos trabajo  varias veces. Si aún no lo tienes en tu google drive, por favor, descargalo desde: \n",
        "\n",
        "https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection\n",
        "\n",
        "\n",
        "La siguiente celda contiene el código para cargar un dataset que está almacenado en tu google drive: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7ikBSY4UQYt"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from datasets import load_dataset\n",
        "\n",
        "# mount your google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# we load the dataset of sarcasm\n",
        "path = \"/content/drive/My Drive/Colab Notebooks/data/sarcasm/\"\n",
        "\n",
        "data_files = {\"train\": path+\"train.csv\", \n",
        "              \"val\":path+\"val.csv\", \n",
        "              \"test\": path+\"test.csv\"}\n",
        "\n",
        "dict_sarcasm = load_dataset(\"csv\", data_files=data_files)\n",
        "dict_sarcasm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bBSV-b0WD5e"
      },
      "source": [
        "También es posible cargar un dataset que está en un repositorio remoto. Mira el siguiente ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQHy_dKQWHsQ"
      },
      "outputs": [],
      "source": [
        "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
        "data_files = {\n",
        "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
        "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
        "}\n",
        "load_dataset(\"json\", data_files=data_files, field=\"data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9nLhipZHyhQ"
      },
      "source": [
        "## Leer a los datos del dataset\n",
        "\n",
        "Ya hemos visto algunos ejemplos de cómo leer los registros del dataset. Simplemente hay que acceder al split y luego indicar el índice del registro que quieres consultar.\n",
        "\n",
        "Por ejemplo, del dataset de sarcasmo, vamos a recuperar la primera instancia del training, y la última del test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yaxE7XKWdgg"
      },
      "outputs": [],
      "source": [
        "print(\"Primera instancia del training:\", dict_sarcasm['train'][0])\n",
        "# Primera instancia del training: {'Unnamed: 0': 23789, 'headline': 'the great vanishing', 'is_sarcastic': 0}\n",
        "\n",
        "print(\"Última instancia de test:\", dict_sarcasm['test'][-1])\n",
        "# Última instancia de test: {'Unnamed: 0': 22846, 'headline': 'supreme court justice sotomayor continues duties after breaking shoulder', 'is_sarcastic': 0}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO0R9-l_MhV4"
      },
      "source": [
        "You can access a specific feature of a specific instance:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptHsSjwUL70m"
      },
      "source": [
        "También podemos tomar directamente los valores de un determinado campo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VGeoa40L_AT"
      },
      "outputs": [],
      "source": [
        "dataset = dict_sarcasm['train']\n",
        "texts = dataset['headline']\n",
        "labels = dataset['is_sarcastic']\n",
        "print(\"Primer texto y label del training:\", texts[0], labels[0])\n",
        "# Primer texto y label del training: the great vanishing 0\n",
        "\n",
        "print(\"Último texto y label del training:\", texts[-1], labels[-1])\n",
        "# Último texto y label del training: motorcyclists riding 2-wide in lane right next to you probably know what they're doing 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJkyek-rM5kX"
      },
      "source": [
        "### Ejercicio 3:\n",
        "¿Crees que le orden de los indices importa cuando estás trabajando con datasets de gran tamaño?. \n",
        "\n",
        "Por ejemplo, dado el objeto dataset que hemos creado anteriormente para contener el split del training del dataset de sarcamo, y las dos opciones para acceder al titular del primer registro: \n",
        "\n",
        "```\n",
        "1) dataset['headline'][0]\n",
        "2) dataset[0]['headline']\n",
        "\n",
        "```\n",
        "¿son igual de eficientes?, en caso contrario, ¿cuál es la opción más eficiente?\n",
        "\n",
        "En la siguiente celda, puedes incluir código para medir el tiempo que tarda el acceso a cada una de las llamadas 1) y 2). Una opción sencilla es usar el paquete **time** y su función **time()**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD-e_7MoRByx"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikwrM9irSQAR"
      },
      "source": [
        "Razona tu respuesta. ¿Por qué una opción es más eficiente que otra?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La segunda opción es más eficiente porque únicamente necesita cargar el primer registro y acceder a su campo 'headline'. Sin embargo, en la primera opción, es necesario recuperar toda la lista de 'headlines', y luego devolver su primer valor. \n",
        "\n",
        "**Por tanto, el orden de los índices sí importan a la hora de acceder a la información.**\n"
      ],
      "metadata": {
        "id": "eUy2w620PDjU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UGonMjNcj9P"
      },
      "source": [
        "## Algunas operaciones interesantes\n",
        "\n",
        "Muchas veces los datasets contiene campos que realmente no vamos a utilizar. \n",
        "En la siguiente celda, mostramos de nuevo  diccionario con los splits del dataset para sarcasmo. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_sarcasm"
      ],
      "metadata": {
        "id": "ZQYKRCLdPw5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El primero de sus campos, 'Unnamed: 0', posiblemente sea el identificador del texto en una base de datos. "
      ],
      "metadata": {
        "id": "Z-AIdXjPQT40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict_sarcasm['train']['Unnamed: 0'])"
      ],
      "metadata": {
        "id": "o-UFXf9KQFy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eliminar campos\n",
        "Este campo no lo vamos a utilizar en nuestro sistema de detección de sarcasmo, y por tanto, podemos eliminarlo del dataset"
      ],
      "metadata": {
        "id": "hW4BrxLcQvY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_sarcasm = dict_sarcasm.remove_columns([\"Unnamed: 0\"])\n",
        "dict_sarcasm"
      ],
      "metadata": {
        "id": "4HYbpvQCQsdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Renombrar campos\n",
        "\n",
        "También es posible renombrar los nombres de los campos. Por ejemplo, vamos a renombrar 'headline' a 'text, y 'is_sarcastic' a 'label'"
      ],
      "metadata": {
        "id": "BxVpzTTjRHsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_sarcasm = dict_sarcasm.rename_column('headline','text')\n",
        "dict_sarcasm = dict_sarcasm.rename_column('is_sarcastic','label')\n",
        "\n",
        "dict_sarcasm"
      ],
      "metadata": {
        "id": "FEjWuX0MQr-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Función filter\n",
        "La función filter() nos va a permitir seleccionar una muestra de nuestro dataset en función de una condición.\n",
        "Por ejemplo, quizá podemos estar interesados en obtener los textos en los que se menciona a Trump (usaremos minúsculas porque los textos del dataset están preprocesados). Mostamos uno de esos textos (de forma aleatoria). \n",
        "In this section, we will work with a dataset that needs to be cleaned. \n",
        "Please, download the [Drug Review Dataset](#https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip) and save its files into your google drive (for example, in 'Colab Notebooks/data/drugsCom_raw')"
      ],
      "metadata": {
        "id": "yK8kTFqJP-0o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGQqpLXVnnN0"
      },
      "outputs": [],
      "source": [
        "\n",
        "word = 'Trump'\n",
        "sample = dict_sarcasm.filter(lambda example: word.lower() in example[\"text\"] )\n",
        "print('Número de ejemplos que contienen:', word, sample)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "# index = random.randint(0,sample['train'].num_rows)\n",
        "index = 0\n",
        "print(sample['train'][index]['text'])\n"
      ],
      "metadata": {
        "id": "TnPl-iBOUAzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPSOLZATsITw"
      },
      "source": [
        "Por ejemplo, ahora de esa muestra, podríamos obtener todos los que ejemplos que se han etiquetado como 1 (sarcástico)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRCwwFDisMm3"
      },
      "outputs": [],
      "source": [
        "new_sample = sample.filter(lambda example: example[\"label\"] == 1)\n",
        "new_sample\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09l77FSow5Qw"
      },
      "outputs": [],
      "source": [
        "# index = 0\n",
        "\n",
        "index = random.randint(0,new_sample['train'].num_rows)\n",
        "print(new_sample['train'][index])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_qBd-DixDzK"
      },
      "source": [
        "Podríamos utilizar la función filter para eliminar todos los registros (ejemplos o instancias) cuyo texto es nulo o está vacío: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_37D0HUexC1K"
      },
      "outputs": [],
      "source": [
        "dict_sarcasm = dict_sarcasm.filter(lambda example: example[\"text\"] is not None \n",
        "                                   and len(example['text'])>0)\n",
        "dict_sarcasm\n",
        "# we only keep those instances whose 'condition' is not None\n",
        "# drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)\n",
        "# drug_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIFJOiRzxjHr"
      },
      "source": [
        "Podemos ver que el dataset no se ha transformado (es decir, no existían registros con textos vacíos). \n",
        "\n",
        "### Usar map\n",
        "\n",
        "Gracias a map podemos aplicar una función a todos los registros del dataset. Por ejemplo, supón que necesitas añadir un tercer campo con la longitud de los textos (número de tokens). En este caso, puedes definir la siguiente función:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zZjqwlVxwVO"
      },
      "outputs": [],
      "source": [
        "def get_length(example):\n",
        "    tokens_example = example[\"text\"].split()\n",
        "    return {\"length\": len(tokens_example)}\n",
        "\n",
        "dict_sarcasm=dict_sarcasm.map(get_length)\n",
        "dict_sarcasm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lez7B0yqzEb1"
      },
      "source": [
        "Veamos como son los ejemplos ahora (tienen tres campos). Fijate en elvalor de length y comprueba si es correcto para ese ejemplo. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y59y9aZC0kU7"
      },
      "outputs": [],
      "source": [
        "dict_sarcasm['train'][1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Cuál es la longitud máxima de los textos?, ¿y su longitud media?"
      ],
      "metadata": {
        "id": "nOHzAm6vXfrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print('tamaño máximo: ', max(dict_sarcasm['train']['length']))\n",
        "print('tamaño medio: ', np.mean(dict_sarcasm['train']['length']))\n",
        "\n"
      ],
      "metadata": {
        "id": "yQM3ENB2XfC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w8gAzrc2ttK"
      },
      "source": [
        "### Ejercicio 4: \n",
        "\n",
        "Recupera los ejemplos (de todos los splits) que tenga más de 20 tokens, ¿cuántos ejemplos hay en cada split?.\n",
        "Muestra algunos ejemplos de cada split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbbGaIKB3K3X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Función sort()\n",
        "La función sort nos permite ordenar los registros del dataset en función de un campo en particular. Por ejemplo, podemos ordenar los registros del training según su longitud (de menor a mayor)"
      ],
      "metadata": {
        "id": "sPYTV8h6YgAJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4nd_U0P2PDH"
      },
      "outputs": [],
      "source": [
        "aux = dict_sarcasm[\"train\"].sort(\"length\")\n",
        "for i in range(5):\n",
        "    print(aux[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostramos el texto más largo:"
      ],
      "metadata": {
        "id": "Oiha8D7cZCNz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9Ty7S5Y2jqy"
      },
      "outputs": [],
      "source": [
        "aux[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfQJMj9l3WCK"
      },
      "source": [
        "Recuerda que si los textos se han tomado de la web, posiblemente contengan muchas etiquetas htmls, que sería interesante eliminar. Afortunadamente, la librería html lo hace por nosotros: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqbKuiMv3eqa"
      },
      "outputs": [],
      "source": [
        "import html\n",
        "\n",
        "text = \"I&#039;m a professor teaching datasets\"\n",
        "html.unescape(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gemqBDJD3p91"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "dict_sarcasm = dict_sarcasm.map(lambda example: {\"text\": html.unescape(example[\"text\"])})\n",
        "print(\"This takes \", time.time()-start, \" seconds\")\n",
        "dict_sarcasm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qrjn6VK4Dsa"
      },
      "source": [
        "El principal problema que tiene la función map es que debe procesar todos los registros a la vez, si la usamos como en el ejemplo anterior. \n",
        "\n",
        "Para aumentar la velocidad, lo recomendable es indicar que la función se va a aplicar en batches. El tamaño de batch se puede configurar, pero su valor por defecto es 1.000 ejemplos. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DstlK0a54Y6Y"
      },
      "outputs": [],
      "source": [
        "# dict_sarcasm = dict_sarcasm.map(lambda example: {\"text\": html.unescape(example[\"text\"])}, batched = True)\n",
        "\n",
        "def clean(example):\n",
        "    cleaned_text = html.unescape(example['text'])\n",
        "    return {\"text\": cleaned_text}\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "drug_dataset = dict_sarcasm.map(clean, batched=True)\n",
        "print(\"This takes \", time.time()-start, \" seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpQ4i7Od5NGZ"
      },
      "source": [
        "Podemos ver que el tiempo de ejecución ha disminuido notablemente. El tiempo se reduce porque ganamos segundos cuando procesamos muchos elementos a la vez en lugar de uno por uno. \n",
        "\n",
        "Esto es importante cuando tenemos muchos registros en el dataset. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnFuxtfYp03E"
      },
      "source": [
        "### Función shuffle y la función select\n",
        "\n",
        "La función select nos permite devolver una muestra de ejemplos pasándole como parámetro la lista de sus índices. \n",
        "\n",
        "La función shuffle permite reordenar de forma aleatoria los ejemplos de un dataset de forma aleatoria. Podemos especificar una semilla para que sea reproducible (siempre devuelva la misma partición). \n",
        "\n",
        "Con el siguiente ejemplo, reordenamos el training del dataset, y seleccionamos los primeros ejemplos:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yej-m93ap0SF"
      },
      "outputs": [],
      "source": [
        "sample = dict_sarcasm[\"train\"].shuffle(seed=42).select(range(5))\n",
        "sample"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(sample.num_rows):\n",
        "    print(sample[i])"
      ],
      "metadata": {
        "id": "2hEu20dHepQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMMnsXT49Rl-"
      },
      "source": [
        "## Creación de splits\n",
        "En algunos datasets, únicamente se proporciona el conjunto de entrenamiento. Para poder entrenar nuestros modelos, validarlos y evaluarlos, mientras que no tengamos un conjunto de evaluación, en algunos casos será necesario, crear nosotros mismo los splits de test, y también si así lo queremos, el conjunto de validación. \n",
        "\n",
        "Para ello, un objeto dataset tiene el métodod **train_test_split**. Veamos un ejemplo.\n",
        "\n",
        "Descarga el dataset el siguiente https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment, que está compuesto por tweets anotados con la opinión para distintas aerolíneas. \n",
        "El dataset únicamente proporciona un fichero. Crea tres splits con el ratio 70:10:20 para training, validation y testing, respectivamente. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we load the twitter airlines dataset. \n",
        "path = \"/content/drive/My Drive/Colab Notebooks/data/airlines/\"\n",
        "# please, specify the right path to the file csv. In my case, it was saved in the folder Colab Notebooks/data/\n",
        "dict_airlines = load_dataset(\"csv\", data_files=path+\"Tweets.csv\")\n",
        "dict_airlines"
      ],
      "metadata": {
        "id": "FWSKB-k3jiMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nif0Gg8v9ZY2"
      },
      "outputs": [],
      "source": [
        "new_dic_airlines = dict_airlines['train'].train_test_split(train_size=0.7, seed=42)\n",
        "new_dic_airlines\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHhQGDIt-BZg"
      },
      "source": [
        "Ya tienes el train (70) y el test (30). Ahora tienes que dividir el test en dos partes: 1/3 parte para validación, y el resto para test. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATg1WNVA95TI"
      },
      "outputs": [],
      "source": [
        "test_val = new_dic_airlines['test'].train_test_split(train_size=0.33, seed=42)\n",
        "test_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpz4ycr3-Fkh"
      },
      "source": [
        "Sustituimos el split test y añadimos el validation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HybMd_Xm-JWy"
      },
      "outputs": [],
      "source": [
        "new_dic_airlines[\"val\"] = test_val['train']\n",
        "new_dic_airlines[\"test\"] = test_val['test']\n",
        "\n",
        "new_dic_airlines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd5Ik56t9ZJ-"
      },
      "source": [
        "Puedes guardarlo en disco\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F20LCMWB8zit"
      },
      "outputs": [],
      "source": [
        "new_dic_airlines.save_to_disk(path+\"tweets\")\n",
        "# it is saved into the folder drug-reviews-clean with a json format\n",
        "print('dataset was saved')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Práctica 1: \n",
        "\n",
        "Con el dataset de los tweets sobre aerolineas, trata de aplicar y reproducir todos lo que hemos visto en este notebook. \n",
        "\n",
        "Además, trata de subir el dataset (una vez limpiado), a tu repositorio de Huggingface (tendrás que darte de alta). Como no queremos tener problemas con los autores del dataset y con Kaggle, deberás crear el dataset como privado. \n",
        "\n",
        "Una vez en tu repositorio de HuggingFace, carga el dataset desde él y trabaja con él. "
      ],
      "metadata": {
        "id": "7Dbah2jgluXr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovysxhoiAAzd"
      },
      "source": [
        "## Práctica 2: \n",
        "\n",
        "Reproduce todo lo aprendido en este notebook para el dataset EXIST. Súbelo a tu repositorio en HuggingFace (como privado). A partir de ahora ya no tendrás que cargarlo desde local, y lo podrás hacer directamente desde huggingface. \n",
        "\n",
        "En esta página, podéis encontrar información: https://huggingface.co/course/chapter5/5?fw=pt#uploading-the-dataset-to-the-hugging-face-hub\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge_h02JiSfEY"
      },
      "source": [
        "\n",
        "Para más información sobre cómo trabajar con datasets en HuggingFace, te recomiendo que consultes la siguiente documentación: \n",
        "https://huggingface.co/course/chapter5/3?fw=pt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6sHK9228lwq"
      },
      "source": [
        "Puedes encontrar más información sobre cómo trabajar con datasets en los siguientes links:\n",
        "- https://huggingface.co/course/chapter5/3?fw=pt.\n",
        "- https://huggingface.co/course/chapter5/5?fw=pt\n",
        "- https://huggingface.co/course/chapter5/5?fw=pt#uploading-the-dataset-to-the-hugging-face-hub\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "OissruTyHAjo",
        "G2985O1T57A6",
        "8ArzlpXEByuQ",
        "ouGQtOQzF6xE",
        "mY3-x-DpGqgP",
        "kYUbc6SuS_L_",
        "jMMnsXT49Rl-",
        "ovysxhoiAAzd",
        "ge_h02JiSfEY"
      ],
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
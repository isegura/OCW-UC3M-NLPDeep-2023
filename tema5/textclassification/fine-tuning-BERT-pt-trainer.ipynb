{"cells":[{"cell_type":"markdown","metadata":{"id":"N4xhgjcMCFD-"},"source":["# Ajustar un modelo pre-entrenado para la clasifición de textos\n","\n","La ** Clasificación de texto ** es una aplicación  muy popular de PLN cuyo objetivo es clasificar un texto con una categoría o etiqueta predefinida. Podemos distinguir entre la **clasificación binaria** donde únicamente hay dos clases, o multiclase donde el conjunto de clases predefinidas son tres o más. La diferencia entre **multiclase** y **multilabel** es que en la primera tarea, un texto es clasificada con una única clase, mientras que en la segunda, un texto puede clasificar con varias etiquetas.\n","\n","En este notebook , estudiaremos cómo ajustar un modelo transformer, en concreto BERT, para la tarea de clasificación de textos. \n","\n","Las ventajas de utilizar un modelo pre-entrenado son las siguientes:\n","- menor coste computacional (vectores ya están pre-entrenados)\n","- reducen el tiempo y esfuerzo porque puedes utlizar modelos sin tener que entrenar desde cero. \n","\n","\n","HuggingFace incluye muchísimos modelos pre-entrenados para muchas tareas de NLP. \n","\n","## ¿Qué es fine-tuning?\n","\n","\n","Fine-tuning es el proceso de usar un modelo pre-entrenado y entrenarlo sobre un dataset para una tarea concreta, como por ejemplo, clasificación de textos o NER. \n","\n","\n","Fuente:\n","https://huggingface.co/docs/transformers/training"]},{"cell_type":"markdown","metadata":{"id":"Fto0gjPZD60u"},"source":["Instalamos las librerías que vamos a necesitar:"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8261,"status":"ok","timestamp":1678221847636,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"},"user_tz":-60},"id":"ULuW4vDnF75z","outputId":"af652b0f-58ca-44d6-ebe3-edb0b120d954"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.10.1)\n","Requirement already satisfied: tokenizers!=0.11.3,\u003c0.14,\u003e=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: pyarrow\u003e=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: fsspec[http]\u003e=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n","Requirement already satisfied: responses\u003c0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.4)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.2.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n","Requirement already satisfied: dill\u003c0.3.7,\u003e=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp-\u003edatasets) (6.0.4)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp-\u003edatasets) (1.3.3)\n","Requirement already satisfied: charset-normalizer\u003c4.0,\u003e=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp-\u003edatasets) (3.0.1)\n","Requirement already satisfied: async-timeout\u003c5.0,\u003e=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp-\u003edatasets) (4.0.2)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp-\u003edatasets) (1.8.2)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp-\u003edatasets) (22.2.0)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp-\u003edatasets) (1.3.1)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.11.0-\u003etransformers) (4.5.0)\n","Requirement already satisfied: chardet\u003c5,\u003e=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests-\u003etransformers) (4.0.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests-\u003etransformers) (2022.12.7)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests-\u003etransformers) (1.26.14)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.8/dist-packages (from requests-\u003etransformers) (2.10)\n","Requirement already satisfied: pytz\u003e=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas-\u003edatasets) (2022.7.1)\n","Requirement already satisfied: python-dateutil\u003e=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas-\u003edatasets) (2.8.2)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil\u003e=2.7.3-\u003epandas-\u003edatasets) (1.15.0)\n"]}],"source":["!pip install transformers datasets "]},{"cell_type":"markdown","metadata":{"id":"G04qZsMwD-md"},"source":["## Data\n","\n","Como dataset vamos a utilizar uno proporcionado por HuggingFace, **trec** (https://huggingface.co/datasets/trec), que es una colección de preguntas que han sido clasificadas por el tipo de respuesta que espera. En concreto, las clases son las siguientes (se recogen en el campo **coarse_label**). \n","\n","- 'ABBR' (0): la respuesta esperada es una abreviatura.\n","- 'ENTY' (1): la respuesta esperada es una entidad.\n","- 'DESC' (2): la respuesta esperada es una descripción.\n","- 'HUM' (3): la respuesta esperada es una persona.\n","- 'LOC' (4): la respuesta esperada es un lugar.\n","- 'NUM' (5): la respuesta esperada es un valor numérico.\n","\n","El dataset incluye un segundo campo, **fine_label**, donde se da una clasificación más fina del tipo de respuesta esperada para cada pregunta. En este notebook, nos centraremos únicamente en la clasificación basada en **coarse_label**.\n","\n","El dataset es distribuido con dos splits: train y test.\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":267},"executionInfo":{"elapsed":2329,"status":"ok","timestamp":1678221849960,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"},"user_tz":-60},"id":"4SHWKjg9FqYW","outputId":"74e00f69-cc40-4f13-ee87-7672eb5769e0"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:datasets.builder:Found cached dataset trec (/root/.cache/huggingface/datasets/trec/default/2.0.0/f2469cab1b5fceec7249fda55360dfdbd92a7a5b545e91ea0f78ad108ffac1c2)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c32a8f192c8a44feb10c21821627a18e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'coarse_label', 'fine_label'],\n","        num_rows: 5452\n","    })\n","    test: Dataset({\n","        features: ['text', 'coarse_label', 'fine_label'],\n","        num_rows: 500\n","    })\n","})"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","dict_dataset = load_dataset(\"trec\")\n","dict_dataset\n"]},{"cell_type":"markdown","metadata":{"id":"8CBMJw1SdTZq"},"source":["Borramos el campo **fine_label**, porque no lo vamos a usar, y renombramos **coarse_label** a **label**"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1678221849960,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"},"user_tz":-60},"id":"j6T3_KnZdZf0","outputId":"b64995ae-05d3-4cad-ab1a-5d0f01e1d9e7"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 5452\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 500\n","    })\n","})"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["dict_dataset = dict_dataset.remove_columns(['fine_label'])\n","dict_dataset = dict_dataset.rename_column('coarse_label','label')\n","\n","dict_dataset"]},{"cell_type":"markdown","metadata":{"id":"BXNVaaBcGHG-"},"source":["El dataset contiene 5452 instancias para training, y 500 para test. \n","Utilizaremos este dataset porque es pequeño y nos permitirá entrenar el modelo en poco tiempo. En HuggingFace, puedes encontrar otros datasets para trabajar en la tarea de clasificación de textos. Por ejemplo, **yelp** (https://huggingface.co/datasets/yelp_review_full) o https://huggingface.co/datasets/sst2\n","\n","También puedes cargar tu propio dataset desde ficheros en local, como hemos visto en notebooks anteriores:"]},{"cell_type":"markdown","metadata":{"id":"w6rgE3mEHXur"},"source":["Vamos a revisar algunas preguntas y sus clases:"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1678221849961,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"},"user_tz":-60},"id":"qxwaUqKmHXH3","outputId":"1785488e-d913-42d5-916a-cd8e09fd50b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["1224 What 's the difference between sleet and freezing rain ? DESC\n","3263 What according to the Kinsey Institute , is the sexual preference of four percent of American males ? DESC\n","89 In which year was New Zealand excluded from the ANZUS alliance ? NUM\n","679 What is a fear of passing high objects ? ENTY\n","2796 What 's destroyed in Genesis 19 : 24 ? ENTY\n","2509 What 's new in the postal world in 1999 ? DESC\n","1684 What wheel did Blaise Pascal invent in a search for perpetual motion ? ENTY\n","3324 How many states have a `` lemon law '' for new automobiles ? NUM\n","3525 Who played Al Jolson in the Jolson Story ? HUM\n","1733 Name the Marvel team loosely based on DC 's Justice League of America ? HUM\n"]}],"source":["TARGET_LABELS = dict_dataset['train'].features['label'].names\n","\n","import random as rn\n","size_training = len(dict_dataset['train'])\n","for i in range(10):\n","    index_random=rn.randint(0,size_training)\n","    random_instance = dict_dataset['train'][index_random]\n","    print(index_random, random_instance['text'], TARGET_LABELS[random_instance['label']])\n"]},{"cell_type":"markdown","metadata":{"id":"FEz-yyTFlcxT"},"source":["### Crear un split para validación. \n","Como el dataset únicamente proporciona dos splits, vamos a tomar el 10% del training como conjunto de validación. "]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1678221849961,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"},"user_tz":-60},"id":"jmDT9cceM9nl","outputId":"1bf572c6-7344-4ee7-9267-23ab53c4b9d0"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 4906\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 546\n","    })\n","})"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["aux = dict_dataset['train'].train_test_split(test_size=0.1)\n","aux"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":268,"status":"ok","timestamp":1678221850205,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"},"user_tz":-60},"id":"UFChjuMkdBFY","outputId":"52a69f0a-2b48-4e33-9093-facc4e8ca523"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 4906\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 500\n","    })\n","    val: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 546\n","    })\n","})"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["dict_dataset['train']=aux['train']\n","dict_dataset['val']=aux['test']\n","del(aux)\n","dict_dataset"]},{"cell_type":"markdown","metadata":{"id":"Rcx_yPPMHK_D"},"source":["### Tokenización\n","\n","Debemos preparar los textos para que tener el formato necesario para la entrada del transformer. Para ello deberemos utilizar el tokenizador asociado al modelo que vayamos a utilizar. \n","Algunos modelos ya tienen clases predefinida para su tokenizador y para tareas específias. Por ejemplo, para cargar el tokenizador de **BERT** es posible utilizar la clase **BertTokenizer**. HuggingFace proporciona una clase que te permite cargar cualquier tokenizador, indicando simplemente el nombre del modelo. Esta clase es **AutoTokenizer**.\n","\n","En este notebook, vamos a utilizar el modelo **bert-base-uncased**. "]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2758,"status":"ok","timestamp":1678221852959,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"},"user_tz":-60},"id":"ibW81YQbION-"},"outputs":[],"source":["from transformers import AutoTokenizer\n","model_name=\"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n"]},{"cell_type":"markdown","metadata":{"id":"sLcZHqFDrbhW"},"source":["Antes de aplciar el tokenizador a todo el dataset, vamos a aplicarlo a un texto:"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1678221852960,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"},"user_tz":-60},"id":"LddXlbDrrdc0","outputId":"02ea198e-77af-462b-cdeb-3760dcf9e9c5"},"outputs":[{"name":"stdout","output_type":"stream","text":["What is the latitude and longitude of El Paso , Texas ?\n","\n"]},{"data":{"text/plain":["{'input_ids': [101, 2054, 2003, 1996, 15250, 1998, 20413, 1997, 3449, 17161, 1010, 3146, 1029, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["sentence = dict_dataset['train'][0]['text']\n","print(sentence)\n","print()\n","encoding = tokenizer(sentence)\n","encoding"]},{"cell_type":"markdown","metadata":{"id":"0N3_3KEfNg51"},"source":["Necesitamos conocer la longitud máximas de las oraciones en el training. En función de esta longitud máxima, podemos establecer el parámetro **MAX_LENGTH** que usaremos para representar las oraciones de nuestro dataset. Todas las oraciones tendrán la misma longitud. \n","Si la longitud máxima es superior a 512 tokens, la limitaremos a 512, porque el tamaño máximo que acepta el modelo BERT es 512.  "]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1095,"status":"ok","timestamp":1678221854051,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"},"user_tz":-60},"id":"sJ1MXZ2QM-0L","outputId":"320d2416-2acb-48c9-9e4b-37d1c1969133"},"outputs":[{"name":"stdout","output_type":"stream","text":["La longitud máxima de la secuencia es:  41\n","max_length 41\n"]}],"source":["MAX_LENGTH= max([len(tokenizer(text).input_ids) for text in dict_dataset['train']['text']])\n","print(\"La longitud máxima de la secuencia es: \", MAX_LENGTH)\n","\n","MAX_LENGTH = min(512, MAX_LENGTH)\n","print(\"max_length\", MAX_LENGTH)\n"]},{"cell_type":"markdown","metadata":{"id":"-uRNuZFEJHgi"},"source":["Vamos a crear una función que nos permita aplicar el tokenizador por lotes, lo que permitirá hacerlo en menos tiempo. \n","El método **map** de Datasets nos permitirá aplicarlo a todo el dataset:\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":307},"executionInfo":{"elapsed":1202,"status":"ok","timestamp":1678221855243,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"},"user_tz":-60},"id":"td6fcYcUJOvk","outputId":"a9a58936-d2c0-476d-9a32-d0c79e337fe6"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e3272dd990244dca28a7f7e1f53e19c","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/4906 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/trec/default/2.0.0/f2469cab1b5fceec7249fda55360dfdbd92a7a5b545e91ea0f78ad108ffac1c2/cache-ac48d0255fce93bb.arrow\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4c8c85d389e24c4b89f4c691734428ca","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/546 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 4906\n","    })\n","    test: Dataset({\n","        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 500\n","    })\n","    val: Dataset({\n","        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 546\n","    })\n","})"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["def tokenize(examples):\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n","\n","\n","data_encodings=dict_dataset.map(tokenize, batched=True)\n","data_encodings"]},{"cell_type":"markdown","metadata":{"id":"J_i3SSvQiRlZ"},"source":["Fijate en las longitudes de las secuencias una vez codificadas. Todas tienes la misma longitud.  "]},{"cell_type":"markdown","metadata":{"id":"0Kv7-mILibO5"},"source":["### Ejercicio: \n","Prueba la funcion **tokenize** con los siguientes cambios:\n","- ¿Por qué si quitas truncation no hay ninguna modificación en la longitud de las oraciones?\n","- No uses el atributo padding, ¿cuál es la longitud de las oraciones?\n","- No uses el atributo max_length, ¿cuál es la longitud de las oraciones?\n"]},{"cell_type":"markdown","metadata":{"id":"h_T3iwOhNx1x"},"source":["## Modelo \n","\n","Los textos ya están preparados!!!\n","\n","Ahora debemos entrenar el modelo pre-entrenado para nuestra tarea. \n","\n","Para entrenar, podemos elegir dos framworks concretos: **tensorflow** o **pytorch**.  En este notebook, usaremos **pytorch**, que en general es más sencillo que **tensorflow**.\n","\n","**Pytorch** proporciona una clase **AutoModelForSequenceClassification**, que ya carga un  transformer adaptado para dicha tarea. Esta clase permite cargar cualquier transformer, aunque muchos de ellos ya incluyen una clase específica para esta tarea. Por ejemplo, para BERT, es posible utilizar la clase **BertForSequenceClassification**. En realidad, **AutoModelForSequenceClassification** llama a la clase **BertForSequenceClassification**.\n","\n","En esta clase, únicamente es necesario indicar el nombre del transformer, y el **número de clases**, en el argumento, **num_labels**. En nuestro caso, es 6.  \n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9092,"status":"ok","timestamp":1678221864333,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"},"user_tz":-60},"id":"9JBNq8fxOobS","outputId":"9f83aad3-ce44-41c7-c08c-03c4bef8bd9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["TARGET_LABELS: ['ABBR', 'ENTY', 'DESC', 'HUM', 'LOC', 'NUM'] num_labels: 6\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoModelForSequenceClassification\n","\n","TARGET_LABELS = dict_dataset['train'].features['label'].names\n","NUM_LABELS = len(TARGET_LABELS)\n","\n","print('TARGET_LABELS:', TARGET_LABELS, 'num_labels:', NUM_LABELS )\n","\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=NUM_LABELS) "]},{"cell_type":"markdown","metadata":{"id":"MaFmjsJSO3XC"},"source":["La ejecución de la celda anterior Produce un warning informando que algunos de los pesos pre-entrenados no están siendo utilizados y algunos han sido inicializados aleatoriamente. ¡No te preocupes, esto es completamente normal!\n","\n","Los vectores finales del modelo pre-entrenado BERT es sustituido por nuevo encabezado para la clasificación que ha sido inicializado aleatoriamente. Durante el entrenamiento, este vectores se ajustan para la tarea de clasificación, transfiriéndole el conocimiento del modelo pre-entrenado. entrenado."]},{"cell_type":"markdown","metadata":{"id":"ZPr0aTbePLML"},"source":["#### Hyperparameteres\n","\n","Necesitamos definir los hiperparámetros que se emplearán durante el proceso de entrenamiento. Algunos de estos hiperpárametros son el número de epochs para el entrenamiento, el tamaño del lote (batch), learning rate, etc.  Puedes experimentar con estos hiperparámetros para encontrar la configuración óptima para tu tarea. \n","\n","En nuestro caso, trabajaremos con los hipérparametros por defecto. Necesitamos crear un objeto de la clase TrainingArguments, que incluye todos los hiperparámetros (ya inicializados con valores por defecto) que puedes ajustar. "]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":1357,"status":"ok","timestamp":1678221865684,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"},"user_tz":-60},"id":"PrE8OCVCPnBv"},"outputs":[],"source":["from transformers import TrainingArguments\n","args = TrainingArguments(output_dir=\"./outputs\")\n","# args"]},{"cell_type":"markdown","metadata":{"id":"WJKfVj0qbO4e"},"source":["Vamos a modificar algunos de ellos, por ejemplo, el tamaño del batch tanto para training como para evaluación (de 8 a 32). Además, vamos a especificar la estrategia de evaluación a epoch para que nos proporcione los resultados después de cada epoch. \n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1678221865685,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"},"user_tz":-60},"id":"BZWFZgDgbMHF"},"outputs":[],"source":["args.evaluation_strategy=\"epoch\"\n","args.per_device_train_batch_size = 32\n","args.per_device_eval_batch_size = 32\n"]},{"cell_type":"markdown","metadata":{"id":"6BDZuYEmQlsM"},"source":["#### Métricas\n","\n","También tenemos que definir el conjunto de métricas que se utilizaran para evaluar el modelo sobre el conjunto de validación. Este conjunto de métricas depende de cada tarea. En el caso de la clasificación de textos, además del accuracy, es interesante conocer la precisión, recall y f1. \n","\n","Vamos a definir una función que compute estas métricas. \n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":586,"status":"ok","timestamp":1678221866265,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"},"user_tz":-60},"id":"yy1MU6W6Q-Hl"},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","\n","def compute_metrics(pred):\n","    \"\"\"recibe un lote prediciones inferidas por el modelo. \"\"\"\n","    y_true = pred.label_ids # son las labels en el gold standard\n","    y_pred = pred.predictions.argmax(-1) # pred.predictions devuelve una lista con las predicciones\n","                                        # para casda clase. Debemos quedarnos con la de mayor probabilidad.\n","\n","    # como son varias clases, utilizaremos la macro\n","    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n","    acc = accuracy_score(y_true, y_pred)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }"]},{"cell_type":"markdown","metadata":{"id":"_Hj7Om7obd1S"},"source":["#### Trainer\n","\n","Ya estamos listos para entrenar el modelo. Para ello, Pytorch ya nos proporciona una clase **Trainer**,  que está optimizada para entrenar transformers, y que además nos va a ahorrar mucho trabajo (no será necesario escribir el ciclo de entrenamiento por epochs y calcular las métricas sobre el conjunto de validación). \n","\n","Para crear este objeto Trainer, deberemos pasarle el modelo, los argumentos, la función para compuar las métricas, y el conjunto de entrenamiento y validación."]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":5026,"status":"ok","timestamp":1678221871286,"user":{"displayName":"ISABEL SEGURA BEDMAR","userId":"10362143810849156637"},"user_tz":-60},"id":"BibvOXZ9bnGY"},"outputs":[],"source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model,            # modelo que será ajustado\n","    args = args,     # hiperparámetros\n","    train_dataset=data_encodings['train'], # conjunto training\n","    eval_dataset=data_encodings['val'],   # conjunto de validación\n","    compute_metrics=compute_metrics,    # función para computar las métricas\n",")"]},{"cell_type":"markdown","metadata":{"id":"diN_CU18b8vR"},"source":["Por fin, podemos entrenar para ajustar el modelo a la tarea de clasificación de textos: "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":293},"id":"1edWWjFUbqSa"},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 4906\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed \u0026 accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 462\n","  Number of trainable parameters = 109486854\n"]},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='26' max='462' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [ 26/462 00:11 \u003c 03:28, 2.09 it/s, Epoch 0.16/3]\n","    \u003c/div\u003e\n","    \u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n"," \u003ctr style=\"text-align: left;\"\u003e\n","      \u003cth\u003eEpoch\u003c/th\u003e\n","      \u003cth\u003eTraining Loss\u003c/th\u003e\n","      \u003cth\u003eValidation Loss\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\u003cp\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"a47q2iZ7dMC0"},"source":["Una vez finalizado el entrenamiento, es posible evaluar el modelo final sobre el conjunto de datos de validación. Las métricas obtenidas son: "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C3d0PThcdMMu"},"outputs":[],"source":["trainer.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"2QIFr6audqR7"},"source":["## Evaluación\n","\n","Por último, vamos a utilizar el modelo para predecir las clases para textos que no han sido utilizados durane el entrenamiento. Es decir, vamos a aplicar el modelo sobre el conjunto test.\n","\n","La siguiente función recibe un texto y devuelve la clase inferida por el modelo. La función codifica el texto usando el tokenizador y el modelo es aplicado sobre esta codificación. Sobre la salida del modelo, aplicaremos una función softmax, que calcule la probabilidad de cada clase. Finalmente, devolvemos la clase con mayor probabilidad (usaremos la función **argmax** .\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3rAM5vigeAa5"},"outputs":[],"source":["def get_prediction(text):\n","    # prepara el texto, aplicamos la misma tokenización que la utilizada en el training\n","    inputs = tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(\"cuda\")\n","    # aplicamos el modelo\n","    outputs = model(**inputs)\n","    # obtenemos la probabilidad para cada clase\n","    probs = outputs[0].softmax(1)\n","    # argmax nos devuelve la clase con mayor probabilidad.\n","    # argmax devuelve un tensor. Debemos devolver su valor asociado \n","    return probs.argmax().item()"]},{"cell_type":"markdown","metadata":{"id":"08YUmfQ1koBk"},"source":["Vamos a aplicar la función **get_prediction** al conjunto de evaluación:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WI88rtXmj_9J"},"outputs":[],"source":["y_pred=[get_prediction(text) for text in dict_dataset['test']['text']]\n"]},{"cell_type":"markdown","metadata":{"id":"FC8RwAYqkHgW"},"source":["Mostramos los resultados finales. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uhbOrTOOkKp-"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","print(classification_report(y_true=dataset['test']['label'], y_pred=y_pred, target_names=TARGET_LABELS))"]},{"cell_type":"markdown","metadata":{"id":"0-3JqK0az3vI"},"source":["### Ejercicio: \n","Busca en hugginface modelos apropiados para la tarea de clasificación de textos.\n","Cambio el valor de la variable **model_name** con nombres de otros modelos, por ejemplo:\n","- bert-base-cased\n","- distilbert-base-uncased\n","- albert-base-v2\n","- xlm-roberta-base\n","\n","¿cuál obtiene mejores resultados?, ¿cuál es más rápido?\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN9B4H6n7MCPrxaHLJhUzEm","name":"","toc_visible":true,"version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0e3272dd990244dca28a7f7e1f53e19c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8e64dd8b89ce45479f5b25a8ff52f5a9","IPY_MODEL_6fdc21424c2041938a96f47441aa6243","IPY_MODEL_80c2ee59ccab484e82d20cd83214a5e8"],"layout":"IPY_MODEL_68633e3433df4848adce01f12b50b2a7"}},"2debb8f7be514150a27fc8d9281bfa5f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4068d0144d834a5794ddf8a86f1c101a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"480e479da22b46149a9ad5de1472ad73":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4c5777bd11d44a06bc1600a1400e8b7c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5ac202dde3f41729373773e919a7683","placeholder":"​","style":"IPY_MODEL_eaec5d01eebc43e9969d00687142f797","value":" 546/546 [00:00\u0026lt;00:00, 4329.26 examples/s]"}},"4c8c85d389e24c4b89f4c691734428ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_877ef16a91bc41bfaa0b1abdbe7e1c9d","IPY_MODEL_c10cded4d75d4b0f9cfc1c8a01bea9a1","IPY_MODEL_4c5777bd11d44a06bc1600a1400e8b7c"],"layout":"IPY_MODEL_d4d275d748d14a3bbb68c5bc203c2e94"}},"5021f7009893478c9ab33f5ad74d2db0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60bc9ee9c8234ace804aa8f64f0bfc86":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65fa40b1c2b741268678777f6d552d52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa69f124cfbb453abd9237cc4608b6ab","placeholder":"​","style":"IPY_MODEL_c76e7541abd841c390832c21c162d557","value":" 2/2 [00:00\u0026lt;00:00, 51.38it/s]"}},"68633e3433df4848adce01f12b50b2a7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"69f98702be074c81a7c78b45c460842a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fdc21424c2041938a96f47441aa6243":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_efe16b2ac1ba47bdac30d13cf2dfd3d6","max":4906,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d9d94ea42f324541be653bc666170903","value":4906}},"7b24e8d8d5c64620aecbdce7e23941a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"80c2ee59ccab484e82d20cd83214a5e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69f98702be074c81a7c78b45c460842a","placeholder":"​","style":"IPY_MODEL_c80c951f621c417395995f40b753cf3c","value":" 4906/4906 [00:01\u0026lt;00:00, 4949.06 examples/s]"}},"849b5b35d58c41b1bc7ffbecf0cd77f6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"877ef16a91bc41bfaa0b1abdbe7e1c9d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7ba3a4790ee498ea4f32da37060eeae","placeholder":"​","style":"IPY_MODEL_60bc9ee9c8234ace804aa8f64f0bfc86","value":"Map: 100%"}},"8e64dd8b89ce45479f5b25a8ff52f5a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_849b5b35d58c41b1bc7ffbecf0cd77f6","placeholder":"​","style":"IPY_MODEL_5021f7009893478c9ab33f5ad74d2db0","value":"Map: 100%"}},"95a800789f4640bcaf2ad4b0062194d4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a915ad5bcec5414682ed3600cfa0f0d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b6aac1e276df49b094c85b02c56cb111":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4068d0144d834a5794ddf8a86f1c101a","placeholder":"​","style":"IPY_MODEL_a915ad5bcec5414682ed3600cfa0f0d7","value":"100%"}},"c0a1493c79154690aaa8c9ec7e0c585c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c10cded4d75d4b0f9cfc1c8a01bea9a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0a1493c79154690aaa8c9ec7e0c585c","max":546,"min":0,"orientation":"horizontal","style":"IPY_MODEL_480e479da22b46149a9ad5de1472ad73","value":546}},"c32a8f192c8a44feb10c21821627a18e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b6aac1e276df49b094c85b02c56cb111","IPY_MODEL_da8a51858da14f70b945dcc1d115f0fb","IPY_MODEL_65fa40b1c2b741268678777f6d552d52"],"layout":"IPY_MODEL_2debb8f7be514150a27fc8d9281bfa5f"}},"c76e7541abd841c390832c21c162d557":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c80c951f621c417395995f40b753cf3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4d275d748d14a3bbb68c5bc203c2e94":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"d7ba3a4790ee498ea4f32da37060eeae":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9d94ea42f324541be653bc666170903":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"da8a51858da14f70b945dcc1d115f0fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_95a800789f4640bcaf2ad4b0062194d4","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7b24e8d8d5c64620aecbdce7e23941a1","value":2}},"eaec5d01eebc43e9969d00687142f797":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"efe16b2ac1ba47bdac30d13cf2dfd3d6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5ac202dde3f41729373773e919a7683":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa69f124cfbb453abd9237cc4608b6ab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
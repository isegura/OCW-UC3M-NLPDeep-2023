{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/47/Acronimo_y_nombre_uc3m.png\"/>\n",
        "\n",
        "<img src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" width=15%/>\n",
        "</center> "
      ],
      "metadata": {
        "id": "W1lB2A-3VuRF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "371qH94rqkn0"
      },
      "source": [
        "# ¿Cómo añadir más capas a continuación del transformer? \n",
        "\n",
        "En los notebooks anteriores, hemos trabajado con modelos transformers cuya arquitectura había sido extendida para tareas concretas, como por ejemplo la clasificación de textos. Dicha arquitectura ha sido encapsulada en una clase de HuggingFace, por ejemplo, **AutoModelForSequenceClassification**. \n",
        "\n",
        "En este notebook, aprenderemos a ajustar el modelo a una tarea, modificando u añadiendo nuevas capas específicas para la tarea sobre el modelo trasnformer. \n",
        "Además, también vamos a aprender cómo obtener los vectores del modelos transformer. \n",
        "\n",
        "Cuando tenemos muy pocos datos en el conjunto de entrenamiento, o si nuestra tarea es específica a un dominio, podríamos usar el conocimiento de otros modelos entrenados para otras tareas pero dentro del mismo dominio. Por ejemplo, supongamos que tenemos un conjunto de entrenamiento para la tarea de análisis de sentimiento en el dominio financiero, y que hay una gran cantidad de modelos que han sido entrenados para la tarea de QA en dicho dominio. La idea de es que podemos podemos aprovechar algunas capas de estos modelos para mejorar nuestra tarea de sentiment analysis. Otro ejemplo es por ejemplo si hemos entrenado un modelo multiclasificación de noticias falsas sobre el dataset Fakeddit (un dataset enorme). Dicho modelo podría ser utilizado para clasificar fake news en otro conjunto de datos cuando queremos aprender a clasificar (binaria) noticias falsas entrenadas en el conjunto de datos Fakkedit (un conjunto de datos enorme). Lo que haríamos sería aprovechar las capas de la tarea de multiclasificación, agregando nuestra propia capa para la clasificación binaria. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5JzZ2vbtIdS"
      },
      "source": [
        "## Transfer learning de analisis de sentimiento a detección de sarcamos. \n",
        " \n",
        " A continuación, vamos a utilizar un modelo que fue entrenado para clasificar tweets en 5 emociones. Vamos a añadir capas adicionales para dirigir una nueva tarea, la detección de sarcamos, que es un problema de clasificación binaria. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg7x9fFNqkQ6"
      },
      "outputs": [],
      "source": [
        "! pip install transformers datasets evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq-EhGX4uQCT"
      },
      "source": [
        "Cargamos el modelo entrenado para la tarea de análisis de sentimiento. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEcxKuyWuPeG"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-emotion\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# 'RobertaTokenizerFast' no tiene el atributo 'model_max_len', así que debemos definirlo\n",
        "tokenizer.model_max_len=512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIhhZvGXutMT"
      },
      "source": [
        "Cargamos ahora el dataset para la tarea de detección de sarcasmos: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73LrKSVqvFe-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "PATH_DATA = '/content/drive/My Drive/Colab Notebooks/data/sarcasm/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meqXF9R5uvWW"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict, load_dataset\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "dataset_dict = load_dataset(\"csv\", data_files={'train':PATH_DATA+\"train.csv\", \"val\":PATH_DATA+\"val.csv\", \"test\":PATH_DATA+\"test.csv\"})\n",
        "dataset_dict\n",
        "\n",
        "dataset_dict = dataset_dict.remove_columns(['Unnamed: 0'])\n",
        "dataset_dict = dataset_dict.rename_columns({'headline':'text','is_sarcastic':'label'})\n",
        "\n",
        "# now, we encoding the data\n",
        "def tokenize(batch):\n",
        "  return tokenizer(batch[\"text\"], truncation=True, max_length=512)\n",
        "\n",
        "encoded_dataset = dataset_dict.map(tokenize, batched=True)\n",
        "\n",
        "# creamos un datacollator que pasará los datos al modelo \n",
        "encoded_dataset.set_format(\"torch\",columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "encoded_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlxLpBJnyS82"
      },
      "source": [
        "Ahora, vamos a crear e nuestro propio modelo. Primero vamos a extraer la cabeza del modelo entrenado para la tarea de análisis de sentimento. Luego vamos a añadir una capa adicional para la tarea de clasificación binaria en la detección de sarcasmo. \n",
        "\n",
        "Creamos una clase, **CustomModel**, cuyo constructor recibe como parámetro el nombre del modelo y el número de labels. El constructor va a extraer el cuerpo de dicho modelo usando la clase **AutoModel**.\n",
        "\n",
        "\n",
        "\n",
        "Luego, usamos **TokenClassifierOutput** para asegurarnos de que la salida sea similar a la salida del modelo original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGQ1TB7Rwmi3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoConfig\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "import torch\n",
        "import torch.nn as nn       #layes for NN\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "\n",
        "  def __init__(self,checkpoint,num_labels): \n",
        "\n",
        "    super(CustomModel,self).__init__() \n",
        "    self.num_labels = num_labels \n",
        "\n",
        "    self.model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
        "    self.dropout = nn.Dropout(0.1) \n",
        "    self.classifier = nn.Linear(768,num_labels) \n",
        "\n",
        "  def forward(self, input_ids=None, attention_mask=None,labels=None):\n",
        "    #utiliza el modelo para generar la salida\n",
        "    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    #aplica el resto de capas\n",
        "    sequence_output = self.dropout(outputs[0]) #outputs[0]=último estado\n",
        "    logits = self.classifier(sequence_output[:,0,:].view(-1,768)) # calcula el error\n",
        "    \n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "      loss_fct = nn.CrossEntropyLoss()\n",
        "      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "    \n",
        "    return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFGdeDos0N7F"
      },
      "source": [
        "Vamos a entrenar el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeKoRF0I0YVL"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model=CustomModel(checkpoint= model_name, num_labels=2).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPU9wg6h0hGj"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    encoded_dataset[\"train\"], shuffle=True, batch_size=32, collate_fn=data_collator\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    encoded_dataset[\"val\"], batch_size=32, collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iV6ijlh400tI"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW,get_scheduler\n",
        "import evaluate \n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "print(num_training_steps)\n",
        "\n",
        "metric = evaluate.load(\"f1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kH5xlR4e0SHx"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar_train = tqdm(range(num_training_steps))\n",
        "progress_bar_eval = tqdm(range(num_epochs * len(eval_dataloader)))\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  for batch in train_dataloader:\n",
        "      batch = {k: v.to(device) for k, v in batch.items()}\n",
        "      outputs = model(**batch)\n",
        "      loss = outputs.loss\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      lr_scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "      progress_bar_train.update(1)\n",
        "\n",
        "  model.eval()\n",
        "  for batch in eval_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "    progress_bar_eval.update(1)\n",
        "    \n",
        "  print(metric.compute())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKhqH4Rq1DvJ"
      },
      "source": [
        "Evaluamos ahora sobre el conjunto test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q450qqT1Gd8"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    encoded_dataset[\"test\"], batch_size=32, collate_fn=data_collator\n",
        ")\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "metric.compute()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/47/Acronimo_y_nombre_uc3m.png\"/>\n",
        "\n",
        "<img src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" width=15%/>\n",
        "</center> "
      ],
      "metadata": {
        "id": "v480ZbBmHuLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T5\n",
        "\n",
        "Excepto el transformer original descrito en el paper \"Attention is all you need\", el resto de modelos que hemos estudiado únicamente se componen de encoders (BERT)  o decoders (GPT). \n",
        "\n",
        "[T5](#https://dl.acm.org/doi/abs/10.5555/3455716.3455856)[1] es un modelo transformador con una arquitectura Seq2Seq, es decir, compuesta por un encoder y un decoder. Precisamente como T5 está basado en una arquitectura Seq2Seq, sigue un enfoque text-to-text, es decir, va a recibir un texto, y va a generar un nuevo texto como salida para cualquier tarea de NLP, aunque dicha el objetivo de dicha tarea no sea generación texto (por ejemplo, clasificación de textos, multietiquetado o NER). \n",
        "\n",
        "El método utilizado para entrenar el modelo T5 es muy similar al utilizado por BERT (enmascaramiento de tokens), pero en T5 no solo se enmascaran tokens sino también secuencias de tokens contiguos. Así la predición es una secuencia de tokens que contiene las respuestas (representada por \"sentinel tokens\"). \n",
        "\n",
        "T5 se entrena usando **teacher forcing**, es decir, el modelo siempre va a recibir la secuencia de entrada (representada con los input_ids) seguida de la secuencia de salida u objetivo (la que debe aprender). Las dos secuencias se separan por un token de secuencia de inicio. La secuencia de salida se alimenta al decodificador utilizando decoder_input_ids.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1iPG7UxZPvy6c2iwixQXYetpTzcHiKDuW' width='50%'>\n",
        "\n",
        "\n",
        "T5 fue entrenado con el corpus “Colossal Clean Crawled Corpus”, que contiene unos ~750GB of de textos en inglés y extraídos de la red (limpiados de etiquetas html). Este dataset es mucho más grande que el usado para entrenar BERT (solo 13 GB de textos), o los 126GBd de textos usados para entrenar XLNet.\n",
        "\n",
        "Posimplemente por el gran tamaño del corpus usado para pre-entrenar T5, este transformer ha conseguido resultados excelentes en muchas tareas de NLP, superando los obtenidos por enfoques previos [1]. T5 es efectiva tanto para tareas de generación de texto como para tareas de clasificación. \n",
        "\n",
        "<img src='https://drive.google.com/uc?id=15XQ-H7IdT3DVtbL7fgwYIm08OYWbB8VZ' width='75%'>\n",
        "\n",
        "Cuando T5 es ajustado a alguna tarea concreta, en la entrada hay que específica el nombre de la tarea, además de la secuencia de entrada. Por ejemplo, si nuestro objetivo es traducir una oración, será necesario indicar la tarea \"translate\" y los idiomas de origen y destino \"from English to French\". \n",
        "Si es una tarea de sentiment analysis (binaria), indicarás \"cola sentence\". \n",
        "\n",
        "\n",
        "[1] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1), 5485-5551."
      ],
      "metadata": {
        "id": "W7_v3i2XA9R4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Resumen:\n",
        "- T5 es un transformer basado en Seq2Seq (encoder-decoder), y considera todos las tareas de NLP como una tarea de texto a texto. \n",
        "- T5 obtiene buenos resultados tanto para tareas de generación de textos como de clasificación. \n",
        "- Es necesario pasar un préfijo para cada tarea. Por ejemplo, para generación de resúmenes, usaremos \"summarize\". \n",
        "\n",
        "- En el encoder, el padding puede hacerse tanto a la izquierda como a la derecha. Como en BERT, las posiciones son absolutas (con respecto al primer token) el padding debe realizarse preferiblemente a la derecha. GPT fue entrenado sin padding. \n",
        "\n",
        "- T5 es distribuido en varias versiones: t5-small, t5-base, t5-large, t5-3b, t5-11b.\n",
        "- Los modelos T5 necesitan una tasa de aprendizaje (learning rate) ligeramente más alta que la predeterminada establecida en el Trainer cuando se usa el optimizador AdamW. Por lo general, 1e-4 y 3e-4 funcionan bien para la mayoría de los problemas (clasificación, resumen, traducción, respuesta a preguntas, generación de preguntas). T5 fue pre-entrenado con el optimizador AdaFactor."
      ],
      "metadata": {
        "id": "gOoWbVMVThwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¿Cómo utilizar T5? \n",
        "\n",
        "Vamos a ver cómo podemos usar T5 para resolver algunas tareas básicas de PLN, como:\n",
        "- 1) Predecir tokens. \n",
        "- 2) Traducción automática. \n",
        "\n",
        "T5 necesita un tokenizador especial, por tanto, necesitamos instalar la librería **sentencepiece**:"
      ],
      "metadata": {
        "id": "PGauGb5jTsHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDu3fjrTCjoZ",
        "outputId": "30a404fb-ddca-4dc1-89f0-20b949d0314b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.27.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### T5 para la tarea de modelado de lenguaje basado en enmascaramiento de tokens. \n",
        "\n",
        "Para esta tarea, usaremos la clase predefinida **T5ForConditionalGeneration**. \n",
        "Para indicar qué huecos debe predecir el modelo, usaremos etiquetas del tipo $<extra\\_id\\_0>$, $<extra\\_id\\_1>$, etc. (tantas como huecos).\n",
        "\n",
        "Por ejemplo, dada la oración \"The cute dog walks in the park\" una posible secuencia de entrada enmascarada podría ser: \n",
        "\"The $<extra\\_id\\_0>$ walks in $<extra\\_id_1>$ park\".\n",
        "Vamos a usar el modelo para ver qué tokens predice para esos huecos:"
      ],
      "metadata": {
        "id": "fkssYQPQCOXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park.\", return_tensors=\"pt\").input_ids\n",
        "# usamos el método generate para que produzca la salida\n",
        "# pasamos de ids a palabras\n",
        "sequence_ids = model.generate(input_ids)\n",
        "sequences = tokenizer.batch_decode(sequence_ids)\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPHgmvE2QRk8",
        "outputId": "516bdfc7-7d64-4996-9e57-51182a8d07b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad><extra_id_0> park offers<extra_id_1> the<extra_id_2> park.</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver que la secuencia de salida generada ha sido '$<pad><extra\\_id\\_0>$ park offers$<extra\\_id\\_1>$ the<extra_id_2> park.</s>\n",
        "Es decir: \n",
        "- **park offers\"\n",
        "- ** the**\n",
        "El decoder ha añadido $<pad>$ como inicio de secuencia, y $</s>$ fin de secuencia. \n"
      ],
      "metadata": {
        "id": "NS6N9wG_RRhp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No parece que el modelo haya producido una buena salida. \n",
        "\n",
        "Vamos a medir el error repecto a una posible salida correcta, como \"$<extra\\_id\\_0>$ cute dog $<extra\\_id\\_1>$ the $<extra\\_id\\_2>$\". "
      ],
      "metadata": {
        "id": "4CoOG_uBQ0UV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
        "labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "loss = model(input_ids=input_ids, labels=labels).loss\n",
        "loss.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P803fFo8ChrW",
        "outputId": "b21182a3-3c22-411e-ed65-9e3c4b9e1c4c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.783731698989868"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a ver cómo trabajar para otros ejemplos: "
      ],
      "metadata": {
        "id": "1DWSseEEQOMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(\"The boys <extra_id_0> to <extra_id_1> garden.\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "sequence_ids = model.generate(input_ids)\n",
        "sequences = tokenizer.batch_decode(sequence_ids)\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGwQ6Kz0QXjp",
        "outputId": "366fecea-ce53-410b-b596-823f9c41dfb0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad> The boys are going to the garden.</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(\"The government <extra_id_0> to <extra_id_1> the law.\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "sequence_ids = model.generate(input_ids)\n",
        "sequences = tokenizer.batch_decode(sequence_ids)\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-W_VW1vScVm",
        "outputId": "cd31a5ea-7e5a-4c61-f53e-7f1b36a25c4a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad><extra_id_0> has a right<extra_id_1> enforce<extra_id_2>.</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(\"Women <extra_id_0> less <extra_id_1>\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "sequence_ids = model.generate(input_ids)\n",
        "sequences = tokenizer.batch_decode(sequence_ids)\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjiRLHLES2kI",
        "outputId": "a4062621-42b7-4487-9948-f8d86f6f549d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad><extra_id_0> are<extra_id_1> likely to be able to survive.</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### T5 para traducción automática\n",
        "\n",
        "En este caso, tenenos que añadir el prefijo \"translate X to Y\", donde X e Y son idiomas. El prefijo es necesario porque fue usado durante el pre-entrenamiento de T5. \n"
      ],
      "metadata": {
        "id": "fpxZG2dCL4Vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(\"translate English to French: The house is wonderful.\", return_tensors=\"pt\").input_ids\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "input_ids = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").input_ids\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "input_ids = tokenizer(\"translate English to Spanish: The house is wonderful.\", return_tensors=\"pt\").input_ids\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq6j7hgoUOin",
        "outputId": "31560d45-7564-4aa7-fa52-4982eb6c0eb4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La maison est merveilleuse.\n",
            "Das Haus ist wunderbar.\n",
            "Das Haus ist wunderbar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el ejemplo anterior, solo procesamos un ejemplo. Sin embargo, para entrenar un modelo, debemos usar una colección de ejemplos.\n",
        "\n",
        "En la tokenización, debemos indicar la longitud máxima para la secuencia de entrada y la longitud máxima para la secuencia de destino.\n",
        "\n",
        "Además, la función de pérdida debe ignorar los tokens de relleno, por lo que reemplazamos su id con -100, que es el ignore_index de CrossEntropyLoss.\n"
      ],
      "metadata": {
        "id": "6_cZmmq4NZ_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# the following 2 hyperparameters are task-specific\n",
        "max_source_length = 512\n",
        "max_target_length = 128\n",
        "\n",
        "# Suppose we have the following 2 training examples:\n",
        "task_prefix = \"translate English to Spanish: \"\n",
        "input_sequence_1 = \"Welcome to New York\"\n",
        "input_sequence_2 = \"HuggingFace is a company\"\n",
        "input_sequences = [input_sequence_1, input_sequence_2]\n",
        "\n",
        "output_sequence_1 = \"Bienvenidos a Nueva York\"\n",
        "output_sequence_2 = \"HuggingFace is una empresa\"\n",
        "\n",
        "encoding = tokenizer(\n",
        "    [task_prefix + sequence for sequence in input_sequences],\n",
        "    padding=\"longest\",\n",
        "    max_length=max_source_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
        "\n",
        "# encode the targets\n",
        "target_encoding = tokenizer(\n",
        "    [output_sequence_1, output_sequence_2],\n",
        "    padding=\"longest\",\n",
        "    max_length=max_target_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "labels = target_encoding.input_ids\n",
        "\n",
        "# replace padding token id's of the labels by -100 so it's ignored by the loss\n",
        "labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "# forward pass\n",
        "loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
        "loss.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkJdJoQEOdmX",
        "outputId": "729a731c-102b-4d9c-8b48-5a6780a391b6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.805405855178833"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}

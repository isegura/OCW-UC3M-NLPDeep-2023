{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tipos de transformers\n",
        "\n",
        "\n",
        "El primer modelo transformer fue propuesto en el artículo [\"Attention is all you need\"](#https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf). Este modelo ha servido como base al resto de modelos transformers que han sido propuestos posteriormente. \n",
        "\n",
        "Principalmente, podemos distinguir los siguientes tipos de transformers:\n",
        "\n",
        "- **Modelos auto-regresivos o decodificadores (decoders)**: estos modelos son pre-entrenados con la tarea de predecir el próximo token, dada una secuencia de tokens previos. Su arquitectura se corresponde con el decodificador del transformer original (artículo \"Attention is all you need\"). Usan una máscara de atención sobre cada secuencia, de modo que en cada token, la cabeza de atención solo puede ver los tokens anteriores. Aunque estos modelos pueden ajustarse para muchas tareas de PLN, su aplicación más natural es la generación de texto. GPT es un ejemplo típico de estos modelos. Fue pre-entrenado sobre el conjunto de datos de Book Corpus. GPT-2 es una versión mejorada que fue pre-entrenado con el dataset WebText (páginas web de Reddit).\n",
        "<center>\n",
        "<img src='https://editor.analyticsvidhya.com/uploads/782781__MrDp6w3Xc-yLuCTbco0xw.png' width='75%'>\n",
        "<figcaption>https://medium.com/@antonio.lopardo/the-basics-of-language-modeling-1c8832f21079</figcaption>\n",
        "</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "- **Modelos auto-encoding o codificadores(o encoders)**: estos modelos son pre-entrenados utilizando la tarea de predecir palabras que han sido ocultadas en una secuencia de tokens. En el pre-entrenamiento de estos modelos, algunos tokens de las oraciones de entrada son enmascarados aleatoriamente, y el modelo debe predecir dichos tokens para obtener las oraciones originales. Su arquitectura se corresponde con el modulo codificador del transformer original (artículo \"Attention is all you need\"). La cabeza de atención puede ver todos los tokens de entrada excepto los enmascarados. De esta forma, es posible aprender una representación bidireccional de la oración completa. Este tipo de modelos se puede utilizar en muchas aplicaciones de PLN, pero su aplicación más natural es la clasificación de oraciones o la clasificación de tokens. BERT, ALBERT, Roberta o Distilbert son ejemplos de este tipo de modelos.\n",
        "<center>\n",
        "<img src='https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/MLM.png' width='75%'>\n",
        "<figcaption>https://www.sbert.net/examples/unsupervised_learning/MLM/README.html</figcaption>\n",
        "</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "La única diferencia entre los auto-regresivo y los auto-encoding es el método que se utiliza para pre-entrenar el modelo. Como se ha dicho antes, mientras que los modelos auto-regresivos están entrenados para la tarea de predecir la siguiente palabra a una secuencia de tokens, los modelos de auto-encoding están entrenados para predecir los tokens enmascarados en una secuencia de entrada.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "- **Modelos Seq2seq**: estos modelos conservan la misma arquitectura del transformer original (artículo, \"Attention is all you need\"), es decir, tiene tanto un codificador como un decodificador. Aunque pueden aplicarse a muchas de las tareas de PLN, su aplicación más natural son las tareas de traducción o tareas seq2seq (traducción, generación de resúmenes o sistemas de question answering). T5 y BART son algunos modelos de este tipo. \n",
        "\n",
        "<center>\n",
        "<img src='https://lh4.googleusercontent.com/H8Vu1PZrMcrkAtc85BYlLxHKXzacMyoOF9Je-KPm5mUOBbR5-CTXMzlOP3LrUgA9JlPKMUY5M4SGVsZ3yGHeJziogpXCOdTncO5FO9v8mqn15tcXmO7xHsKLovmwZ-UfW6Lc_DJRhUk' width='75%'>\n",
        "<figcaption>source: http://ruder.io/deep-learning-nlp-best-practices/</figcaption>\n",
        "</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "Por último, los **modelos multimodales** toman como entrada textos y otro tipo de datos (por ejemplo, imágenes). Suelen ser específicos para una tarea determinada. Por ejemplo, MMBT [MMBT](#https://arxiv.org/abs/1909.02950) es un modelo para la tarea de clasificación (noticias falsas multimodales). Este modelo toma como entradas las incrustaciones del texto tokenizado y las activaciones finales de una resnet preentrenada en imágenes (después de la capa de agrupación)."
      ],
      "metadata": {
        "id": "sdku_pFKAOlA"
      }
    }
  ]
}

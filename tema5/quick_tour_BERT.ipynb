{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "P3sXKiS2VZxb"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/47/Acronimo_y_nombre_uc3m.png\"/>\n",
        "\n",
        "<img src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" width=15%/>\n",
        "</center>   \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0WB3BRSmLYfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Un breve tour por BERT\n",
        "\n",
        "BERT, modelo auto-encoding, es probablemente uno de los transformers m치s conocidos. \n",
        "\n",
        "En su pre-entrenamiento se utilizaron dos estrategias: predicci칩n de tokens enmascarados y predecir si dos oraciones son consecutivas. Es un modelo bidireccional. BERT fue pre-entrenado con una gran colecci칩n de textos (BookCorpus y Wikipedia). \n",
        "\n",
        "BERT puede ser facilmente ajustado para cualquier tarea de NLP, a침adiendo simplemente un capa de salida adecuada para la tarea. En su momento, BERT fue capaz de superar los resultados de los sistemas desarrollados hasta el momento en muchas tareas de NLP como la clasificaci칩n de textos o el reconocimiento de entidades. Sin embargo, aunque BERT fue entrenado utilizando una tarea de modelado de lenguaje al predecir tokens enmascarados, BERT no es un modelo apropiado para tareas de generaci칩n de texto. \n",
        "\n",
        "El siguiente [paper](#https://aclanthology.org/N19-1423/) describe en detalle la arquitectura de BERT.\n"
      ],
      "metadata": {
        "id": "eUhZqMO_lK8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En los siguientes notebooks, aprenderemos a ajustar BERT para distintas tareas de NLP. HuggingFace proporciona algunas clases pre-definidas que pueden er utilizados para tareas espec칤ficas. \n",
        "\n",
        "El primer paso ser치 instalar la librer칤a transformers de HuggingFace."
      ],
      "metadata": {
        "id": "doinLOD8Om9d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfO7Jzt4LX2D"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reviewing BERT parameters\n",
        "\n",
        "We can see some properties:\n",
        "- *max_position_embeddings*:512, tama침o de la secuencia m치s larga que BERT puede procesar. \n",
        "- *hidden_size*: 768. Dimensi칩n de los vectores. \n",
        "- *vocab_size*: el vocabulario de BERT tiene un total de 30,522 tokens 칰nicos.\n",
        "- *num_attention_heads*: 12, es el n칰mero de cabezas de atenci칩n en cada capa de atenci칩n.\n",
        "- *num_hidden_layers*: 12, n칰mero de capas ocultas.\n",
        "- *pad_token_id*: 0, es el identificador para el token de padding. \n",
        "- *position_embedding_type*: en BERT, se utiliza una posici칩n absoluta para representar los tokens. .\n",
        "\n",
        "Puedes encontrar m치s informaci칩n en el siguiente [link](#https://huggingface.co/docs/transformers/model_doc/bert)."
      ],
      "metadata": {
        "id": "5Yeksxh6O_Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertConfig\n",
        "\n",
        "# Initializing a BERT bert-base-uncased style configuration\n",
        "configuration = BertConfig()\n",
        "\n",
        "# Initializing a model from the bert-base-uncased style configuration\n",
        "model = BertModel(configuration)\n",
        "\n",
        "# Accessing the model configuration\n",
        "configuration = model.config\n",
        "\n",
        "print(configuration)"
      ],
      "metadata": {
        "id": "sHF8JX8vOTh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BertForNextSentencePrediction\n",
        "Como se ha dicho antes, predecir si dos oraciones eran consecutivas fue una de las estrategias utilizadas para pre-entrenar BERT. \n",
        "La librer칤a transformers nos proporciona una clase que nos permite utilizar  esta estrategia sobre pares de oraciones. La clase recibe como entrada dos oraciones, y devuelve la probabilidad de que esas dos oraciones puedan ocurrir juntas. \n",
        "Antes de utilizar la clase, debemos cargar el tokenizador asociado con el modelo BERT. Esta tokenizador nos permite representar los textos en el formato adecuado que necesita BERT (tokens_ids, token_type_ids, attention_mask).\n"
      ],
      "metadata": {
        "id": "JYUYHLYkWCrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "first_sentence = \"Leonardo da Vinci was an Italian man who lived in the time of the Renaissance.\"\n",
        "second_sentence = \"He is famous for his paintings.\"\n",
        "\n",
        "# the tokenizer transform the two sentences to the input format that BERT needs. \n",
        "encoding = tokenizer(first_sentence, second_sentence, return_tensors=\"pt\")\n",
        "\n",
        "print(encoding)"
      ],
      "metadata": {
        "id": "QbY5ytMNRmq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como ya explicamos en notebooks anteriores, el tokenizador devuelve un diccionario con tres campos: input_ids, token_type_ids, attention_mask. "
      ],
      "metadata": {
        "id": "gU0MVE6zTT5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoding.input_ids)       #ids of the tokens\n",
        "print(encoding.token_type_ids) #0's for tokens of the first sentence, 1's for tokens of the second one\n",
        "print(encoding.attention_mask)  #1's for tokens, 0's for padding. \n"
      ],
      "metadata": {
        "id": "mvWsnNdOS7mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora podemos utilizar la clase **BertForNextSentencePrediction** para calcular la probabilidad.\n",
        "Aplicamos el modelo sobre el encoding de las dos oraciones. \n",
        "\n",
        "El modelo devuelve un objeto vector (Tensor), **logists**, cuyo primer elemento es la probabilidad de que las dos oraciones pueden ocurrir juntas. Podemos ver que obtenemos un valor alto alto que indica que las dos oraciones pueden ocurrir juntas.\n"
      ],
      "metadata": {
        "id": "n6sfu-esTRjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**encoding) \n",
        "logits = outputs.logits\n",
        "print(type(logits))\n",
        "print(logits)\n",
        "\n",
        "# assert logits[0, 0] < logits[0, 1]  # next sentence was random"
      ],
      "metadata": {
        "id": "WtLI7GXXS3uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sin embargo, en el siguiente ejemplo, la probabilidad es negativa, lo que indica que ambas oraciones no es probable que aparezcan juntas. "
      ],
      "metadata": {
        "id": "5A8ytlJwXczg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_sentence = \"Leonardo da Vinci was an Italian man who lived in the time of the Renaissance.\"\n",
        "second_sentence = \"Pizza is an Italian food that was created in Italy.\"\n",
        "\n",
        "# the tokenizer transform the two sentences to the input format that BERT needs. \n",
        "encoding = tokenizer(first_sentence, second_sentence, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model(**encoding)\n",
        "logits = outputs.logits\n",
        "print(logits)\n",
        "# assert logits[0, 0] < logits[0, 1]  # next sentence was random"
      ],
      "metadata": {
        "id": "iINt9eEzUq_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TFBertForMaskedLM\n",
        "\n",
        "Vamos a probar tambi칠n la otra estrategia utilizada para pre-entrenar BERT: predecir un token enmascarado. \n",
        "La librer칤a **transformers** ya incluye una clase, TFBertForMaskedLM, que implementa la tarea. En este caso, la entrada ser치 una oraci칩n donde un token ha sido reemplazado por una m치scara [洧洧냢洧녡洧쬫. "
      ],
      "metadata": {
        "id": "ScU1RVthVxNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertForMaskedLM\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "model = TFBertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "encoding = tokenizer(\"This [MASK] is beautiful.\", return_tensors=\"tf\")\n",
        "\n",
        "print(encoding)"
      ],
      "metadata": {
        "id": "JKpHIK7wVtMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(**encoding)\n",
        "logits = output.logits\n",
        "print(logits)\n"
      ],
      "metadata": {
        "id": "GygYHncuAFyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despu칠s de aplicar el modelo sobre la entrada codificada, este devuelve un tensor  con dimensi칩n (1,7,30522).\n",
        "\n",
        "- donde 7 es el n칰mero de tokens en la oraci칩n de entrada: \"$[CLS]$ This $[MASK]$ is beautiful. $[SEP]$\". Tiene 7 tokens al incluir los tokens especiales $[CLS]$ y $[SEP]$.\n",
        "- 30.522 es el tama침o de vocabulario de BERT.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uATUd3Oenopf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La salida est치 en **logits[0]**, es un tensor (vector) de forma 7 x 30522, cada fila representa un token y cada columna representa una de las 30.522 tokens del vocabulario de BERT. BERT ha calculado la probabilidad del token para cada uno de los tokens del vocabulario, as칤 en la fila i y columna j, el tensor contiene la probabilidad de que el token representado en la fila i sea el token j del vocabulario. \n",
        "Por ejemplo, $logits[0][2]$ va a contener el vector de probabilidades que el modelo ha calculado para el token $[MASK]$ en la oraci칩n \"$[CLS]$ This $[MASK]$ is beautiful.$[SEP]$\". La probabilidad m치s alta se corresponder치 con el token predecido para $[MASK]$. "
      ],
      "metadata": {
        "id": "zJkIPdZk0P18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En primer lugar, vamos a calcular de forma autom치tica el 칤ndice de $[MASK]$ en el vocabulario y en la oraci칩n de entrada. \n"
      ],
      "metadata": {
        "id": "6eyU3KJPqa0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.mask_token_id)\n",
        "# check that the decoding of this id is MASK\n",
        "print(tokenizer.decode(tokenizer.mask_token_id))"
      ],
      "metadata": {
        "id": "XVg3FouE09iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por tanto, $[MASK]$ tiene el 칤ndice 103 en el vocabulario de BERT. \n",
        "Ahora vamos a obtener el 칤ndice de $[MASK]$ en la oraci칩n de entrada: "
      ],
      "metadata": {
        "id": "3dAgUpa51V4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoding.input_ids)\n",
        "print(type(encoding.input_ids))\n",
        "print(type(encoding.input_ids.numpy()))\n",
        "\n",
        "# convert from Tensor to list: tensor -> numpy -> list\n",
        "input_sentence_list = (encoding.input_ids.numpy().tolist())\n",
        "print(input_sentence_list)\n",
        "input_sentence_list = input_sentence_list[0]\n",
        "print(input_sentence_list)\n"
      ],
      "metadata": {
        "id": "cU-925ngpSZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "쮺u치l es la posici칩n del token mask_token_id en la oraci칩n? "
      ],
      "metadata": {
        "id": "r1LMO6mR1j26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find the index \n",
        "mask_token_index = input_sentence_list.index(tokenizer.mask_token_id)\n",
        "mask_token_index\n",
        "print(\"posici칩n de [MASK] en la oraci칩n:\", mask_token_index)\n"
      ],
      "metadata": {
        "id": "J7uKMl1q1mLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora vamos a obtener el tensor que ha calculado BERT para ese token:"
      ],
      "metadata": {
        "id": "gRr7B01ryyRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = logits[0]\n",
        "print(print(type(output)))\n",
        "# from Tensor to numpy to list\n",
        "output_list = output.numpy().tolist()\n",
        "predicted_list_mask = output_list[mask_token_index]\n",
        "print(\"predictions for mask:\", output_list[mask_token_index])\n",
        "\n"
      ],
      "metadata": {
        "id": "C8ejJUrjy-oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculamos el 칤ndice cuyo valore es el mayor del tesnro. Para eso usamos la funci칩n **argmax**:"
      ],
      "metadata": {
        "id": "qNNfPh2c5w7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_id_token_mask = tf.math.argmax(predicted_list_mask, axis=-1)\n",
        "predicted_id_token_mask"
      ],
      "metadata": {
        "id": "Zpbv_f5F52_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para conocer el token que ha inferido el modelo, necesitamos utilizar la funci칩n **decode**: \n"
      ],
      "metadata": {
        "id": "3_B3pwKB4uWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(predicted_id_token_mask)"
      ],
      "metadata": {
        "id": "OexghvLd4ldT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hemos recuperado el token, transformando el tensor a una lista y operando con sus funciones. Sin embargo, en lugar de hacer esto, es recomendable utilizar la operaciones para trabajar con Tensor (un tensor es una matriz multidimensional). \n",
        "Ver: https://www.tensorflow.org/guide/tensor\n",
        "\n",
        "Por ejemplo, for obtener el 칤ndice de $[MASK]$ en la oraci칩n, debemos hacer lo siguiente: "
      ],
      "metadata": {
        "id": "bJ-pQE5w1sUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_token_index = tf.where((encoding.input_ids == tokenizer.mask_token_id)[0])\n",
        "print(mask_token_index)"
      ],
      "metadata": {
        "id": "gkIBo5sA1xnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)\n",
        "\n",
        "# we have to take the maximum value\n",
        "predicted_id_token_mask = tf.math.argmax(predicted_list_mask, axis=-1)\n",
        "#finally we decode to obtain the word\n",
        "tokenizer.decode(predicted_id_token_mask)"
      ],
      "metadata": {
        "id": "nllKddAU4ZuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio: \n",
        "Prueba tu mismo/a con otras oraciones: \"Please, read this $[MASK]$.\""
      ],
      "metadata": {
        "id": "O4wzNO9aMCKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"Please, read this [MASK].\", return_tensors=\"tf\")\n",
        "logits = model(**inputs).logits\n",
        "\n",
        "mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])\n",
        "print(mask_token_index)\n",
        "selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)\n",
        "\n",
        "# we have to take the maximum value\n",
        "predicted_id_token_mask = tf.math.argmax(selected_logits, axis=-1)\n",
        "#finally we decode to obtain the word\n",
        "tokenizer.decode(predicted_id_token_mask)"
      ],
      "metadata": {
        "id": "QiY6CtCI7K17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BertForTokenClassification\n",
        "\n",
        "La librer칤a **transformes** proporciona tambi칠n una clase que permite utilizar el modelo BERT para la tarea de reconocimiento de entidades (NER). NER se puede ver como una tarea donde el objetivo es clasificar cada token en una oraci칩n, y las clases siguen el est치ndar IOB (O no entidad, I- token interno de entidad). \n",
        "\n",
        "Vamos a cargar un modelo de BERT que ha sido ajustaedo a la tarea de NER. "
      ],
      "metadata": {
        "id": "P3sXKiS2VZxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForTokenClassification\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "model = BertForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "\n",
        "inputs = tokenizer(\n",
        "    \"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# we use the model to infer, so we need to disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "# logits"
      ],
      "metadata": {
        "id": "jGpKAiStVTyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_token_class_ids = logits.argmax(-1)\n",
        "print(predicted_token_class_ids)\n",
        "# Note that tokens are classified rather then input words which means that\n",
        "# there might be more predicted token classes than words.\n",
        "# Multiple token classes might account for the same word\n",
        "predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
        "predicted_tokens_classes\n",
        "# ['O', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'I-LOC', 'I-LOC'] "
      ],
      "metadata": {
        "id": "4jSPC_ZLNUnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "BERT necesita codificar la posici칩n de cada token. Esta posici칩n es necesaria para representar c칩mo un token en un posici칩n afecta a otro token en una posici칩n diferente. **BERT utiliza la posici칩n absoluta de cada token** (desde 1 hasta la longitud m치xima de la secuencia). Por este motivo, **el padding en BERT debe aplicarse a la derecha**."
      ],
      "metadata": {
        "id": "XiY6O9eG5prc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En los siguientes notebooks, aprenderemos c칩mo ajustar BERT para la tarea de clasificaci칩n de textos. Tambi칠n estudiaremos otros modelos basados en BERT:  Distilbert, RoBERTa, etc. \n",
        "\n"
      ],
      "metadata": {
        "id": "fvXfgpp5_6rU"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "P3sXKiS2VZxb"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/47/Acronimo_y_nombre_uc3m.png\"/>\n",
        "\n",
        "<img src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" width=15%/>\n",
        "</center>   \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0WB3BRSmLYfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Un breve tour por BERT\n",
        "\n",
        "BERT, modelo auto-encoding, es probablemente uno de los transformers más conocidos. \n",
        "\n",
        "En su pre-entrenamiento se utilizaron dos estrategias: predicción de tokens enmascarados y predecir si dos oraciones son consecutivas. Es un modelo bidireccional. BERT fue pre-entrenado con una gran colección de textos (BookCorpus y Wikipedia). \n",
        "\n",
        "BERT puede ser facilmente ajustado para cualquier tarea de NLP, añadiendo simplemente un capa de salida adecuada para la tarea. En su momento, BERT fue capaz de superar los resultados de los sistemas desarrollados hasta el momento en muchas tareas de NLP como la clasificación de textos o el reconocimiento de entidades. Sin embargo, aunque BERT fue entrenado utilizando una tarea de modelado de lenguaje al predecir tokens enmascarados, BERT no es un modelo apropiado para tareas de generación de texto. \n",
        "\n",
        "El siguiente [paper](#https://aclanthology.org/N19-1423/) describe en detalle la arquitectura de BERT.\n"
      ],
      "metadata": {
        "id": "eUhZqMO_lK8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En los siguientes notebooks, aprenderemos a ajustar BERT para distintas tareas de NLP. HuggingFace proporciona algunas clases pre-definidas que pueden er utilizados para tareas específicas. \n",
        "\n",
        "El primer paso será instalar la librería transformers de HuggingFace."
      ],
      "metadata": {
        "id": "doinLOD8Om9d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfO7Jzt4LX2D"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reviewing BERT parameters\n",
        "\n",
        "We can see some properties:\n",
        "- *max_position_embeddings*:512, tamaño de la secuencia más larga que BERT puede procesar. \n",
        "- *hidden_size*: 768. Dimensión de los vectores. \n",
        "- *vocab_size*: el vocabulario de BERT tiene un total de 30,522 tokens únicos.\n",
        "- *num_attention_heads*: 12, es el número de cabezas de atención en cada capa de atención.\n",
        "- *num_hidden_layers*: 12, número de capas ocultas.\n",
        "- *pad_token_id*: 0, es el identificador para el token de padding. \n",
        "- *position_embedding_type*: en BERT, se utiliza una posición absoluta para representar los tokens. .\n",
        "\n",
        "Puedes encontrar más información en el siguiente [link](#https://huggingface.co/docs/transformers/model_doc/bert)."
      ],
      "metadata": {
        "id": "5Yeksxh6O_Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertConfig\n",
        "\n",
        "# Initializing a BERT bert-base-uncased style configuration\n",
        "configuration = BertConfig()\n",
        "\n",
        "# Initializing a model from the bert-base-uncased style configuration\n",
        "model = BertModel(configuration)\n",
        "\n",
        "# Accessing the model configuration\n",
        "configuration = model.config\n",
        "\n",
        "print(configuration)"
      ],
      "metadata": {
        "id": "sHF8JX8vOTh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BertForNextSentencePrediction\n",
        "Como se ha dicho antes, predecir si dos oraciones eran consecutivas fue una de las estrategias utilizadas para pre-entrenar BERT. \n",
        "La librería transformers nos proporciona una clase que nos permite utilizar  esta estrategia sobre pares de oraciones. La clase recibe como entrada dos oraciones, y devuelve la probabilidad de que esas dos oraciones puedan ocurrir juntas. \n",
        "Antes de utilizar la clase, debemos cargar el tokenizador asociado con el modelo BERT. Esta tokenizador nos permite representar los textos en el formato adecuado que necesita BERT (tokens_ids, token_type_ids, attention_mask).\n"
      ],
      "metadata": {
        "id": "JYUYHLYkWCrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "first_sentence = \"Leonardo da Vinci was an Italian man who lived in the time of the Renaissance.\"\n",
        "second_sentence = \"He is famous for his paintings.\"\n",
        "\n",
        "# the tokenizer transform the two sentences to the input format that BERT needs. \n",
        "encoding = tokenizer(first_sentence, second_sentence, return_tensors=\"pt\")\n",
        "\n",
        "print(encoding)"
      ],
      "metadata": {
        "id": "QbY5ytMNRmq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como ya explicamos en notebooks anteriores, el tokenizador devuelve un diccionario con tres campos: input_ids, token_type_ids, attention_mask. "
      ],
      "metadata": {
        "id": "gU0MVE6zTT5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoding.input_ids)       #ids of the tokens\n",
        "print(encoding.token_type_ids) #0's for tokens of the first sentence, 1's for tokens of the second one\n",
        "print(encoding.attention_mask)  #1's for tokens, 0's for padding. \n"
      ],
      "metadata": {
        "id": "mvWsnNdOS7mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora podemos utilizar la clase **BertForNextSentencePrediction** para calcular la probabilidad.\n",
        "Aplicamos el modelo sobre el encoding de las dos oraciones. \n",
        "\n",
        "El modelo devuelve un objeto vector (Tensor), **logists**, cuyo primer elemento es la probabilidad de que las dos oraciones pueden ocurrir juntas. Podemos ver que obtenemos un valor alto alto que indica que las dos oraciones pueden ocurrir juntas.\n"
      ],
      "metadata": {
        "id": "n6sfu-esTRjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**encoding) \n",
        "logits = outputs.logits\n",
        "print(type(logits))\n",
        "print(logits)\n",
        "\n",
        "# assert logits[0, 0] < logits[0, 1]  # next sentence was random"
      ],
      "metadata": {
        "id": "WtLI7GXXS3uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sin embargo, en el siguiente ejemplo, la probabilidad es negativa, lo que indica que ambas oraciones no es probable que aparezcan juntas. "
      ],
      "metadata": {
        "id": "5A8ytlJwXczg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_sentence = \"Leonardo da Vinci was an Italian man who lived in the time of the Renaissance.\"\n",
        "second_sentence = \"Pizza is an Italian food that was created in Italy.\"\n",
        "\n",
        "# the tokenizer transform the two sentences to the input format that BERT needs. \n",
        "encoding = tokenizer(first_sentence, second_sentence, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model(**encoding)\n",
        "logits = outputs.logits\n",
        "print(logits)\n",
        "# assert logits[0, 0] < logits[0, 1]  # next sentence was random"
      ],
      "metadata": {
        "id": "iINt9eEzUq_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TFBertForMaskedLM\n",
        "\n",
        "Vamos a probar también la otra estrategia utilizada para pre-entrenar BERT: predecir un token enmascarado. \n",
        "La librería **transformers** ya incluye una clase, TFBertForMaskedLM, que implementa la tarea. En este caso, la entrada será una oración donde un token ha sido reemplazado por una máscara [𝑀𝐴𝑆𝐾]. "
      ],
      "metadata": {
        "id": "ScU1RVthVxNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertForMaskedLM\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "model = TFBertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "encoding = tokenizer(\"This [MASK] is beautiful.\", return_tensors=\"tf\")\n",
        "\n",
        "print(encoding)"
      ],
      "metadata": {
        "id": "JKpHIK7wVtMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(**encoding)\n",
        "logits = output.logits\n",
        "print(logits)\n"
      ],
      "metadata": {
        "id": "GygYHncuAFyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Después de aplicar el modelo sobre la entrada codificada, este devuelve un tensor  con dimensión (1,7,30522).\n",
        "\n",
        "- donde 7 es el número de tokens en la oración de entrada: \"$[CLS]$ This $[MASK]$ is beautiful. $[SEP]$\". Tiene 7 tokens al incluir los tokens especiales $[CLS]$ y $[SEP]$.\n",
        "- 30.522 es el tamaño de vocabulario de BERT.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uATUd3Oenopf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La salida está en **logits[0]**, es un tensor (vector) de forma 7 x 30522, cada fila representa un token y cada columna representa una de las 30.522 tokens del vocabulario de BERT. BERT ha calculado la probabilidad del token para cada uno de los tokens del vocabulario, así en la fila i y columna j, el tensor contiene la probabilidad de que el token representado en la fila i sea el token j del vocabulario. \n",
        "Por ejemplo, $logits[0][2]$ va a contener el vector de probabilidades que el modelo ha calculado para el token $[MASK]$ en la oración \"$[CLS]$ This $[MASK]$ is beautiful.$[SEP]$\". La probabilidad más alta se corresponderá con el token predecido para $[MASK]$. "
      ],
      "metadata": {
        "id": "zJkIPdZk0P18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En primer lugar, vamos a calcular de forma automática el índice de $[MASK]$ en el vocabulario y en la oración de entrada. \n"
      ],
      "metadata": {
        "id": "6eyU3KJPqa0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.mask_token_id)\n",
        "# check that the decoding of this id is MASK\n",
        "print(tokenizer.decode(tokenizer.mask_token_id))"
      ],
      "metadata": {
        "id": "XVg3FouE09iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por tanto, $[MASK]$ tiene el índice 103 en el vocabulario de BERT. \n",
        "Ahora vamos a obtener el índice de $[MASK]$ en la oración de entrada: "
      ],
      "metadata": {
        "id": "3dAgUpa51V4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoding.input_ids)\n",
        "print(type(encoding.input_ids))\n",
        "print(type(encoding.input_ids.numpy()))\n",
        "\n",
        "# convert from Tensor to list: tensor -> numpy -> list\n",
        "input_sentence_list = (encoding.input_ids.numpy().tolist())\n",
        "print(input_sentence_list)\n",
        "input_sentence_list = input_sentence_list[0]\n",
        "print(input_sentence_list)\n"
      ],
      "metadata": {
        "id": "cU-925ngpSZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Cuál es la posición del token mask_token_id en la oración? "
      ],
      "metadata": {
        "id": "r1LMO6mR1j26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find the index \n",
        "mask_token_index = input_sentence_list.index(tokenizer.mask_token_id)\n",
        "mask_token_index\n",
        "print(\"posición de [MASK] en la oración:\", mask_token_index)\n"
      ],
      "metadata": {
        "id": "J7uKMl1q1mLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora vamos a obtener el tensor que ha calculado BERT para ese token:"
      ],
      "metadata": {
        "id": "gRr7B01ryyRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = logits[0]\n",
        "print(print(type(output)))\n",
        "# from Tensor to numpy to list\n",
        "output_list = output.numpy().tolist()\n",
        "predicted_list_mask = output_list[mask_token_index]\n",
        "print(\"predictions for mask:\", output_list[mask_token_index])\n",
        "\n"
      ],
      "metadata": {
        "id": "C8ejJUrjy-oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculamos el índice cuyo valore es el mayor del tesnro. Para eso usamos la función **argmax**:"
      ],
      "metadata": {
        "id": "qNNfPh2c5w7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_id_token_mask = tf.math.argmax(predicted_list_mask, axis=-1)\n",
        "predicted_id_token_mask"
      ],
      "metadata": {
        "id": "Zpbv_f5F52_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para conocer el token que ha inferido el modelo, necesitamos utilizar la función **decode**: \n"
      ],
      "metadata": {
        "id": "3_B3pwKB4uWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(predicted_id_token_mask)"
      ],
      "metadata": {
        "id": "OexghvLd4ldT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hemos recuperado el token, transformando el tensor a una lista y operando con sus funciones. Sin embargo, en lugar de hacer esto, es recomendable utilizar la operaciones para trabajar con Tensor (un tensor es una matriz multidimensional). \n",
        "Ver: https://www.tensorflow.org/guide/tensor\n",
        "\n",
        "Por ejemplo, for obtener el índice de $[MASK]$ en la oración, debemos hacer lo siguiente: "
      ],
      "metadata": {
        "id": "bJ-pQE5w1sUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_token_index = tf.where((encoding.input_ids == tokenizer.mask_token_id)[0])\n",
        "print(mask_token_index)"
      ],
      "metadata": {
        "id": "gkIBo5sA1xnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)\n",
        "\n",
        "# we have to take the maximum value\n",
        "predicted_id_token_mask = tf.math.argmax(predicted_list_mask, axis=-1)\n",
        "#finally we decode to obtain the word\n",
        "tokenizer.decode(predicted_id_token_mask)"
      ],
      "metadata": {
        "id": "nllKddAU4ZuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio: \n",
        "Prueba tu mismo/a con otras oraciones: \"Please, read this $[MASK]$.\""
      ],
      "metadata": {
        "id": "O4wzNO9aMCKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"Please, read this [MASK].\", return_tensors=\"tf\")\n",
        "logits = model(**inputs).logits\n",
        "\n",
        "mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])\n",
        "print(mask_token_index)\n",
        "selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)\n",
        "\n",
        "# we have to take the maximum value\n",
        "predicted_id_token_mask = tf.math.argmax(selected_logits, axis=-1)\n",
        "#finally we decode to obtain the word\n",
        "tokenizer.decode(predicted_id_token_mask)"
      ],
      "metadata": {
        "id": "QiY6CtCI7K17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BertForTokenClassification\n",
        "\n",
        "La librería **transformes** proporciona también una clase que permite utilizar el modelo BERT para la tarea de reconocimiento de entidades (NER). NER se puede ver como una tarea donde el objetivo es clasificar cada token en una oración, y las clases siguen el estándar IOB (O no entidad, I- token interno de entidad). \n",
        "\n",
        "Vamos a cargar un modelo de BERT que ha sido ajustaedo a la tarea de NER. "
      ],
      "metadata": {
        "id": "P3sXKiS2VZxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForTokenClassification\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "model = BertForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "\n",
        "inputs = tokenizer(\n",
        "    \"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# we use the model to infer, so we need to disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "# logits"
      ],
      "metadata": {
        "id": "jGpKAiStVTyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_token_class_ids = logits.argmax(-1)\n",
        "print(predicted_token_class_ids)\n",
        "# Note that tokens are classified rather then input words which means that\n",
        "# there might be more predicted token classes than words.\n",
        "# Multiple token classes might account for the same word\n",
        "predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
        "predicted_tokens_classes\n",
        "# ['O', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'I-LOC', 'I-LOC'] "
      ],
      "metadata": {
        "id": "4jSPC_ZLNUnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "BERT necesita codificar la posición de cada token. Esta posición es necesaria para representar cómo un token en un posición afecta a otro token en una posición diferente. **BERT utiliza la posición absoluta de cada token** (desde 1 hasta la longitud máxima de la secuencia). Por este motivo, **el padding en BERT debe aplicarse a la derecha**."
      ],
      "metadata": {
        "id": "XiY6O9eG5prc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En los siguientes notebooks, aprenderemos cómo ajustar BERT para la tarea de clasificación de textos. También estudiaremos otros modelos basados en BERT:  Distilbert, RoBERTa, etc. \n",
        "\n"
      ],
      "metadata": {
        "id": "fvXfgpp5_6rU"
      }
    }
  ]
}
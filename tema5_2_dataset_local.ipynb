{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isegura/OCW-UC3M-NLPDeep-2023/blob/main/tema5_2_dataset_local.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/47/Acronimo_y_nombre_uc3m.png\" width=50%/>\n",
        "\n",
        "<h1><font color='#12007a'>Procesamiento de Lenguaje Natural con Aprendizaje Profundo</font></h1>\n",
        "<p>Autora: Isabel Segura Bedmar</p>\n",
        "\n",
        "<img align='right' src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" width=15%/>\n",
        "</center>   "
      ],
      "metadata": {
        "id": "2VxRFBBezfa7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAV7hBMZ3bIB"
      },
      "source": [
        "# 5.2. Cómo cargar un dataset en un objeto Dataset (Hugging Face)\n",
        "En el ejercicio anterior aprendimos a cargar un dataset alojado en Hugging Face (https://huggingface.co/datasets) usando la librería **datasets**.\n",
        "\n",
        "En este ejercicio, veremos cómo es posible además cargar un dataset almacenado localmente o en un servidor y guardarlo en un objeto **Dataset** de la librería **datasets**. Es cierto, que podríamos seguir cargandolo en un dataframe de Pandas, pero para trabajar con transformers, utilizar las clases **DatasetDict** y **Dataset** de la librería **datasets** nos faciliará mucho el trabajo a la hora de procesar los datos y pasarselos al modelo.\n",
        "\n",
        "Además, también aprenderemos a trabajar con algunos métodos útiles para trabajar con dataset, tales como **filter**, **map**, **sort**, **select** y **shuffle**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYUbc6SuS_L_"
      },
      "source": [
        "En este ejercicio, vamos a utlizar un dataset proporcionado por Kaggle para la tarea de detección de sacarmo. Puedes encontrarlo en el siguiente enlace:\n",
        "\n",
        "https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection\n",
        "\n",
        "El dataset está formado por títulos de noticias de periódicos online, y cada uno de estos títulos ha sido clasificado con 1, si el título usa el sarcasmo, y 0, si el título no usa el sarcasmo.\n",
        "\n",
        "\n",
        "Descarga los tres ficheros del dataset y guardalos en tu carpeta 'Colab Notebooks/data/sarcasm/' de tu unidad de google drive.\n",
        "\n",
        "\n",
        "**NOTA PARA PODER EJECUTAR ESTE NOTEBOOK**:\n",
        "\n",
        "1) Para poder ejercutar correctamente este notebook, deberás abrirlo en tu Google Drive (por ejemplo, en la carpeta 'Colab Notebooks').\n",
        "\n",
        "2) Además, debes guardar el dataset en tu Google Drive, dentro de carpeta 'Colab Notebooks/data/sarcasm/'."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Montar unidad de google drive y cambiar directorio de trabajo\n",
        "\n",
        "Como ya hemos hecho en otros ejercicioss, lo primero que tenemos que hacer es montar nuestra unidad de google drive y modificar el directorio de trabajo actual para que sea el directorio donde está almacenado el dataset:\n",
        "\n"
      ],
      "metadata": {
        "id": "EgYB7y4S2KkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/data/sarcasm/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpM6m5M82QoB",
        "outputId": "25dc08c2-996f-43f1-a987-b13914a99d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la siguiente celda, comprobamos que en efecto la carpeta contiene los tres ficheros que hemos descargado de Kaggle:"
      ],
      "metadata": {
        "id": "z5KkYeh43s_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aeih18I9FvjO",
        "outputId": "0c4fb5c0-30aa-4b61-a2c4-aad626c11ad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test.csv  train.csv  val.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMm-ZCT32rgM"
      },
      "source": [
        "## Instalación de la librería datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1A__GPoP3h11"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargar el dataset desde local\n",
        "\n",
        "Ahora ya sí estamos preparados para cargar el dataset. Para ello simplemente, como primer argumento de la función **load_dataset**, debemos pasar la cadena **'csv'**. De esta forma, estamos indicando que vamos a cargar el dataset desde local y en formato csv. Como segundo argumento, vamos a pasarle un diccionario **data_files**, cuyas claves son los nombres de los splits que vamos a crear (en nuestro caso son tres: train, validation y test), y asociado a cada clave, la ruta al fichero que contiene las instancias de dicho split. Como en la primera celda ya modificamos el directorio de trabajo actual a la carpeta que almacena los tres ficheros, bastará con indicar el nombre de cada fichero (es decir, su ruta relativa)."
      ],
      "metadata": {
        "id": "A0IP1p41DHeT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7ikBSY4UQYt",
        "outputId": "e9ecff5b-d970-4c36-b9b3-580ffb50c6a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['Unnamed: 0', 'headline', 'is_sarcastic'],\n",
              "        num_rows: 19952\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['Unnamed: 0', 'headline', 'is_sarcastic'],\n",
              "        num_rows: 2850\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['Unnamed: 0', 'headline', 'is_sarcastic'],\n",
              "        num_rows: 5701\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data_files = {\"train\": \"train.csv\",\n",
        "              \"validation\": \"val.csv\",\n",
        "              \"test\": \"test.csv\"}\n",
        "\n",
        "dict_sarcasm= load_dataset(\"csv\", data_files=data_files)\n",
        "dict_sarcasm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como resultado podemos ver que se ha creado un objeto **DatasetDict** que es un diccionario con tres claves, una para cada split: train, validation y test.\n",
        "Podemos ver que el número de filas (**num_rows**), que son el número de instancias de cada split, es distinto para cada uno de ellos, siendo el más grande el de train (con 19.952 instancias) y el más pequeño el de validación, sólo con 2.850.\n",
        "\n",
        "Además, también podemos ver que cada instancia está formada por tres campos (features):\n",
        "- 'Unnamed: 0': es una especie de identificador, pero es un campo que realmente no va a ser utilizado.\n",
        "- 'headline': texto del titular\n",
        "- 'is_sarcastic': 0, si el titular no es sarcástico, 1 en otro caso.\n",
        "\n"
      ],
      "metadata": {
        "id": "jtPxV1Px42Bp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bBSV-b0WD5e"
      },
      "source": [
        "## Cargar dataset desde un remoto\n",
        "También podemos cargar un dataset alojado un servidor. Otra vez deberemos indicar el formato del dataset (consulta el siguiente enlace para ver qué formatos están permitidos https://huggingface.co/docs/datasets/loading) , en el caso del ejemplo **json**, y un diccionario con los ficheros que componen el dataset, indicando para cada uno el nombre del split y su url.\n",
        "\n",
        "En la celda siguiente vamos a cargar el dataset Stanford Question Answering Dataset (SQUAD) para la tarea de question answering. El dataset se compone de pares de textos, formado por una pregunta y su respuesta.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQHy_dKQWHsQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "584f1886-3619-4164-831e-489da174a4e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['paragraphs', 'title'],\n",
              "        num_rows: 442\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['paragraphs', 'title'],\n",
              "        num_rows: 48\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
        "data_files = {\n",
        "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
        "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
        "}\n",
        "load_dataset(\"json\", data_files=data_files, field=\"data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9nLhipZHyhQ"
      },
      "source": [
        "## Cómo acceder a la información\n",
        "\n",
        "Recuerda que para acceder a los datos de una instancia, tendrás que indicar primero su split, el índice de la instancia, y luego el campo que quieras consultar. Vamos a ver algunos ejemplos sobre el dataset para la tarea de detección de sarcasmo que cargamos al principio:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yaxE7XKWdgg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53e1b3a8-6a70-434d-a0a8-59fc9e8e51db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primera instancia del training: {'Unnamed: 0': 23789, 'headline': 'the great vanishing', 'is_sarcastic': 0}\n",
            "Última instancia de test: {'Unnamed: 0': 22846, 'headline': 'supreme court justice sotomayor continues duties after breaking shoulder', 'is_sarcastic': 0}\n"
          ]
        }
      ],
      "source": [
        "print(\"Primera instancia del training:\", dict_sarcasm['train'][0])\n",
        "# Primera instancia del training: {'Unnamed: 0': 23789, 'headline': 'the great vanishing', 'is_sarcastic': 0}\n",
        "\n",
        "print(\"Última instancia de test:\", dict_sarcasm['test'][-1])\n",
        "# Última instancia de test: {'Unnamed: 0': 22846, 'headline': 'supreme court justice sotomayor continues duties after breaking shoulder', 'is_sarcastic': 0}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJkyek-rM5kX"
      },
      "source": [
        "\n",
        "Las siguientes dos instrucciones devolverán el mismo resultado (el título de la primera instancia del split train):\n",
        "\n",
        "```\n",
        "1) dict_sarcasm['train']['headline'][0]\n",
        "2) dict_sarcasm['train'][0]['headline']\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "¿son igual de eficientes?, en caso contrario, ¿cuál es la opción más eficiente?\n",
        "\n",
        "En la siguiente celda, puedes incluir código para medir el tiempo que tarda el acceso a cada una de las llamadas 1) y 2). Una opción sencilla es usar el paquete **time** y su función **time()**:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta primera opción, para consultar el primer título, accedemos primero al campo headline del train, y luego a la instancia con índice 0:"
      ],
      "metadata": {
        "id": "FtxK0UoIIURR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD-e_7MoRByx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14ab4e63-b46c-4ecc-dd78-c4f6a82987ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtener dict_sarcasm[\"train\"][\"text\"][0] tardó:  0.03412818908691406 segundos\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "dict_sarcasm[\"train\"][\"headline\"][0]\n",
        "end = time.time()\n",
        "\n",
        "time_required_feature_index = end - start\n",
        "\n",
        "print('Obtener dict_sarcasm[\"train\"][\"text\"][0] tardó: ', time_required_feature_index, 'segundos')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta segunda opción, para acceder al primer título, accedemos a la primera de las instancias del split train, y luego a su campo headline"
      ],
      "metadata": {
        "id": "vI34ne1hIfB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "start = time.time()\n",
        "dict_sarcasm['train'][0][\"headline\"]\n",
        "end = time.time()\n",
        "time_required_index_feature = end - start\n",
        "\n",
        "print('Obtener dict_sarcasm[\"train\"][0][\"text\"] tardó: ', time_required_index_feature, 'segundos')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxUY_arnGvls",
        "outputId": "61a8ca40-9ed0-4982-8868-86a80cc56b58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtener dict_sarcasm[\"train\"][0][\"text\"] tardó:  0.0016179084777832031 segundos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikwrM9irSQAR"
      },
      "source": [
        "¿Por qué la segunda opción es más eficiente? Es más eficiente porque únicamente necesita cargar el primer registro y acceder a su campo 'headline'. Sin embargo, en la primera opción, es necesario recuperar toda la lista de 'headlines', y luego devolver su primer valor.\n",
        "\n",
        "**Por tanto, el orden de los campos a la hora de acceder a la información sí es importante.**\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "OissruTyHAjo",
        "G2985O1T57A6",
        "8ArzlpXEByuQ",
        "ouGQtOQzF6xE",
        "mY3-x-DpGqgP",
        "kYUbc6SuS_L_",
        "jMMnsXT49Rl-",
        "ovysxhoiAAzd",
        "ge_h02JiSfEY"
      ],
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNqw9UcRY/jaayY9c2beB+S",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
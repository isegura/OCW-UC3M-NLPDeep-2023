{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isegura/OCW-UC3M-NLPDeep-2023/blob/main/tema1_2_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/47/Acronimo_y_nombre_uc3m.png\" width=50%/>\n",
        "\n",
        "<h1><font color='#12007a'>Procesamiento de Lenguaje Natural con Aprendizaje Profundo</font></h1>\n",
        "<p>Autora: Isabel Segura Bedmar</p>\n",
        "\n",
        "<img align='right' src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" width=15%/>\n",
        "</center>  "
      ],
      "metadata": {
        "id": "2M5VO0CpXYOI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZfZs1_I1HSi"
      },
      "source": [
        "# 1.1. Librería NLTK\n",
        "\n",
        "En este ejercicio, trabajaremos con la librería\n",
        "NLTK (Natural Language Toolkit) (https://www.nltk.org/).\n",
        "\n",
        "NLTK es una librería de Python que nos permite trabajar con textos en lenguaje natural.  Esta librería permite  procesar  textos y realizar varias tareas básicas de PLN. En este ejercicio, se estudian las siguientes tareas:\n",
        "- división de oraciones y tokenización\n",
        "- stemming y lematización\n",
        "- análisis morfosintáctico (PoS tagging)\n",
        "- análisis sintáctico (parsing).\n",
        "\n",
        "Además de estas tareas básicas, NLTK también permite reconocer entidades.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOPFoYYF0q1L"
      },
      "source": [
        "No es necesario instalar la librería NLTK (ya está incluida en Google Colab).\n",
        "\n",
        "Sin embargo, si será necesario descargar algunos paquetes para realizar ciertas tareas. Por ejemplo, es necesario descargar el paquete *punkt* necesario para realizar tareas de análisis sintáctico:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDAk269kcCUf",
        "outputId": "1c55c176-cec3-49d8-f958-82ce8867dbf9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRssgMxS09v8"
      },
      "source": [
        "## Qué tareas podemos realizar con NLTK\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Cómo dividir un texto en sus oraciones\n",
        "\n",
        "\n",
        "Para la mayoría de los textos sería suficiente con considerar el signo de puntuación '.' como carácter para dividir el texto. Sin embargo, esta división no siempre es trivial (por ejemplo, qué ocurre con abreviaturas que contienen '.').\n",
        "\n",
        "NLTK proporciona un método, **sent_tokenize**, que segmenta un texto en oraciones de forma robusta, siendo capaz de manejar todo tipo de textos."
      ],
      "metadata": {
        "id": "5NdMjof7t2C6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1KtkYDhifyg",
        "outputId": "d9e5b2c3-afb9-4610-888d-4b8820fa1353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text='''Billy always listens to his mother. He always does what she says.\n",
        "If his mother says, Brush your teeth, Billy brushes his teeth.\n",
        "If his mother says, Go to bed, Billy goes to bed. Billy is a very good boy.\n",
        "His father, Dr. Smith, is very proud of him.'''\n",
        "\n",
        "\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Billy always listens to his mother.\n",
            "He always does what she says.\n",
            "If his mother says, Brush your teeth, Billy brushes his teeth.\n",
            "If his mother says, Go to bed, Billy goes to bed.\n",
            "Billy is a very good boy.\n",
            "His father, Dr. Smith, is very proud of him.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-ssxWRk1HTI"
      },
      "source": [
        "Para poder dividir un texto en otro idioma distinto al inglés, sería necesario cargar el tokenizador para el idioma correspondiente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCoAxDbd1HTK",
        "outputId": "0b75a7c3-fdb1-4d76-bc64-66996a818790",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer_es = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
        "text='Este es un curso de NLP. Ahora estamos estudiando NLTK. Luego veremos Spacy.'\n",
        "sentences=tokenizer_es.tokenize(text)\n",
        "print(sentences)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Este es un curso de NLP.', 'Ahora estamos estudiando NLTK.', 'Luego veremos Spacy.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pMqW0OviksL"
      },
      "source": [
        "\n",
        "\n",
        "### Cómo tokenizar un texto en sus oraciones\n",
        "\n",
        "La tokenización consiste en dividir un texto en sus palabras y signos de puntuación.\n",
        "\n",
        "El método **split** no es capaz de manejar los signos de puntuacion y tampoco ciertos usos del apóstrofo *'* (genitivos sajón, formas cortas de la negación , etc):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSklTFqY1HSl",
        "outputId": "2c4b4e5f-35ad-4ab5-d640-c8e963c0413b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text=\"Mr. O'Neill thinks that the boys' stories about Chile's capital aren't amusing.\"\n",
        "tokens=[t for t in text.split()]\n",
        "print(tokens)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Mr.', \"O'Neill\", 'thinks', 'that', 'the', \"boys'\", 'stories', 'about', \"Chile's\", 'capital', \"aren't\", 'amusing.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErhjCtOP1HSw"
      },
      "source": [
        "\n",
        "NLTK proporciona un método, **word_tokenize**, capaz de realizar la tokenización de un texto de forma más robusta que el método split.\n",
        "\n",
        "Compara la tokenización producida por split y la del método word_tokenize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A9HHd5t1HSx",
        "outputId": "002039a6-6d4b-4f87-8375-f51a20cedb17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokens=nltk.word_tokenize(text)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Mr.', \"O'Neill\", 'thinks', 'that', 'the', 'boys', \"'\", 'stories', 'about', 'Chile', \"'s\", 'capital', 'are', \"n't\", 'amusing', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCPV4be11HTd"
      },
      "source": [
        "### Cómo identificar las categorías morfosintácticas (PoS) de las palabras en un texto\n",
        "\n",
        "Clasificar palabras en sus categorías morfosintácticas o gramáticales (Part-of-speech tags).\n",
        "\n",
        " Estas etiquetas nos proporcionan información muy útil para tareas como el reconocimiento de entidades o la extracción de relaciones.\n",
        "\n",
        " Es necesarios descargar el paquete **averaged_perceptron_tagger**:\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCGY_Pgzv5nR",
        "outputId": "74e99cca-fa5a-41a6-e53e-08798351b77c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez tokenizado un texto, debemos aplicar el **pos_tag** para obtener sus etiquetas morfosintácticas:"
      ],
      "metadata": {
        "id": "bKzmMH_61xCP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0Ypz7qJ1HTe",
        "outputId": "566ac844-c780-4d7f-f730-8d62c725182f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text=\"At least four people were dead after a man began shooting at a synagogue in the Squirrel Hill neighbourhood of Pittsburgh on Saturday.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "tags=nltk.pos_tag(tokens)\n",
        "print(tags)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('At', 'IN'), ('least', 'JJS'), ('four', 'CD'), ('people', 'NNS'), ('were', 'VBD'), ('dead', 'JJ'), ('after', 'IN'), ('a', 'DT'), ('man', 'NN'), ('began', 'VBD'), ('shooting', 'VBG'), ('at', 'IN'), ('a', 'DT'), ('synagogue', 'NN'), ('in', 'IN'), ('the', 'DT'), ('Squirrel', 'NNP'), ('Hill', 'NNP'), ('neighbourhood', 'NN'), ('of', 'IN'), ('Pittsburgh', 'NNP'), ('on', 'IN'), ('Saturday', 'NNP'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El listado completo de las categorías morfosintácticas de NLTK está disponible en:\n",
        "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
        "\n",
        "Además, la siguiente celda también nos permite consultar este listado."
      ],
      "metadata": {
        "id": "FbQthPbLxP3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK9PVWh1xI5m",
        "outputId": "a20b0a53-0473-4f87-d973-7f7048cb7817"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$: dollar\n",
            "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
            "'': closing quotation mark\n",
            "    ' ''\n",
            "(: opening parenthesis\n",
            "    ( [ {\n",
            "): closing parenthesis\n",
            "    ) ] }\n",
            ",: comma\n",
            "    ,\n",
            "--: dash\n",
            "    --\n",
            ".: sentence terminator\n",
            "    . ! ?\n",
            ":: colon or ellipsis\n",
            "    : ; ...\n",
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "CD: numeral, cardinal\n",
            "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
            "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
            "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
            "DT: determiner\n",
            "    all an another any both del each either every half la many much nary\n",
            "    neither no some such that the them these this those\n",
            "EX: existential there\n",
            "    there\n",
            "FW: foreign word\n",
            "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
            "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
            "    terram fiche oui corporis ...\n",
            "IN: preposition or conjunction, subordinating\n",
            "    astride among uppon whether out inside pro despite on by throughout\n",
            "    below within for towards near behind atop around if like until below\n",
            "    next into if beside ...\n",
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n",
            "JJR: adjective, comparative\n",
            "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
            "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
            "    cozier creamier crunchier cuter ...\n",
            "JJS: adjective, superlative\n",
            "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
            "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
            "    dearest deepest densest dinkiest ...\n",
            "LS: list item marker\n",
            "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
            "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
            "    two\n",
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n",
            "NNPS: noun, proper, plural\n",
            "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
            "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
            "    Apache Apaches Apocrypha ...\n",
            "NNS: noun, common, plural\n",
            "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
            "    divestitures storehouses designs clubs fragrances averages\n",
            "    subjectivists apprehensions muses factory-jobs ...\n",
            "PDT: pre-determiner\n",
            "    all both half many quite such sure this\n",
            "POS: genitive marker\n",
            "    ' 's\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "RBR: adverb, comparative\n",
            "    further gloomier grander graver greater grimmer harder harsher\n",
            "    healthier heavier higher however larger later leaner lengthier less-\n",
            "    perfectly lesser lonelier longer louder lower more ...\n",
            "RBS: adverb, superlative\n",
            "    best biggest bluntest earliest farthest first furthest hardest\n",
            "    heartiest highest largest least less most nearest second tightest worst\n",
            "RP: particle\n",
            "    aboard about across along apart around aside at away back before behind\n",
            "    by crop down ever fast for forth from go high i.e. in into just later\n",
            "    low more off on open out over per pie raising start teeth that through\n",
            "    under unto up up-pp upon whole with you\n",
            "SYM: symbol\n",
            "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
            "TO: \"to\" as preposition or infinitive marker\n",
            "    to\n",
            "UH: interjection\n",
            "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
            "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
            "    man baby diddle hush sonuvabitch ...\n",
            "VB: verb, base form\n",
            "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
            "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
            "    boost brace break bring broil brush build ...\n",
            "VBD: verb, past tense\n",
            "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
            "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
            "    speculated wore appreciated contemplated ...\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "VBN: verb, past participle\n",
            "    multihulled dilapidated aerosolized chaired languished panelized used\n",
            "    experimented flourished imitated reunifed factored condensed sheared\n",
            "    unsettled primed dubbed desired ...\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
            "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
            "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
            "WDT: WH-determiner\n",
            "    that what whatever which whichever\n",
            "WP: WH-pronoun\n",
            "    that what whatever whatsoever which who whom whosoever\n",
            "WP$: WH-pronoun, possessive\n",
            "    whose\n",
            "WRB: Wh-adverb\n",
            "    how however whence whenever where whereby whereever wherein whereof why\n",
            "``: opening quotation mark\n",
            "    ` ``\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalización de textos: lematización y stemming\n",
        "\n"
      ],
      "metadata": {
        "id": "-cOO5IF4pILz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lematización\n",
        "\n",
        "La lematización consiste en el análisis morfológico de una palabra para obtener su lema o forma canónica (la palabra que se encuentra en el diccionario).\n",
        "\n",
        "Por ejemplo, 'walk', 'walked', 'walks', 'walking' comparten la misma forma base o lema: 'walk'.\n",
        "\n",
        "La lematización está basada en el uso de diccionario (recursos léxicos). También utiliza la información del contexto de la palabra a lematizar.\n",
        "\n",
        "En NLTK, necesitamos descargar el paquete **wordnet**. WordNet es uno de las bases de datos léxicas del idioma inglés. Contiene información sobre nombres, verbos, adjetivos y adverbios.\n"
      ],
      "metadata": {
        "id": "FE1jCUNM2g_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ymxf8M7OL2Ws",
        "outputId": "e444155d-9f26-4d42-b751-b9677e8c1fc4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clase **WordNetLemmatizer** proporciona el método **lematize** que recibe una palabra y devuelve su lema.\n",
        "\n",
        "En la siguiente celda, NLTK es utilizado para tokenizar una oración. Para cada token, se muestra su lema:"
      ],
      "metadata": {
        "id": "J9K1D4Axv-m_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxsVp0Vn1HTl",
        "outputId": "e267e26e-24d8-472a-f43e-477cb350da87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "sentence='The women sang songs and stories about the thieves.'\n",
        "tokens=nltk.word_tokenize(sentence)\n",
        "lematizer = WordNetLemmatizer()\n",
        "print('Token:\\t\\tLemma:')\n",
        "for t in tokens:\n",
        "    print(t,'\\t\\t',lematizer.lemmatize(t))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token:\t\tLemma:\n",
            "The \t\t The\n",
            "women \t\t woman\n",
            "sang \t\t sang\n",
            "songs \t\t song\n",
            "and \t\t and\n",
            "stories \t\t story\n",
            "about \t\t about\n",
            "the \t\t the\n",
            "thieves \t\t thief\n",
            ". \t\t .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw3TXFyk1HTr"
      },
      "source": [
        "#### Stemming\n",
        "Es el proceso de reducir las palabras flexionadas a su raíz.\n",
        "\n",
        "Está basado en un conjunto de reglas ( [Algoritmo de Porter](https://https://tartarus.org/martin/PorterStemmer/)) que tratan de encontrar la raíz de una palabra.\n",
        "\n",
        "Ejemplos de reglas:\n",
        "\n",
        "| Regla | Ejemplo  |\n",
        "|----------|----------|\n",
        "| S ->     | cats -> cat |\n",
        "| ES ->   | glasses -> glass   |\n",
        "| (verb) ED ->   | wanted -> want   |\n",
        "| (verb) ING ->   | killing -> kill   |\n",
        "\n",
        "NLTK proporciona la clase PorterStemmer, que es una implementación del algoritmo Porter. Veamos el siguiente ejemplo:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRg-6oeF1HTs",
        "outputId": "0844fe66-ed9b-47af-834f-11d3b4594c15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "for word in ['fish', 'fishes', 'fished', 'fishing']:\n",
        "    print(word, ', stem=', stemmer.stem(word))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fish , stem= fish\n",
            "fishes , stem= fish\n",
            "fished , stem= fish\n",
            "fishing , stem= fish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sin embargo, este no siempre funciona correctamente:\n"
      ],
      "metadata": {
        "id": "OMz_Ae8yxHXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in ['stories', 'leaves', 'horses', 'does']:\n",
        "    print(word, ', stem=', stemmer.stem(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6es4ARcCxJ-Y",
        "outputId": "17ec1a6b-4705-4dc0-8824-7cb1be7b1ccd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stories , stem= stori\n",
            "leaves , stem= leav\n",
            "horses , stem= hors\n",
            "does , stem= doe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**¿Qué proceso es más correcto: la lematización o el stemming?**\n",
        "\n",
        "Veamos algunos ejemplos, para ver qué proceso es más robusto:"
      ],
      "metadata": {
        "id": "H9HXKPOcxNH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in ['stories', 'leaves', 'horses', 'does']:\n",
        "    print(word, ', stem=', stemmer.stem(word), ' lema=', lematizer.lemmatize(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iHVhwBVxVf5",
        "outputId": "afc97cb6-1481-452f-a9ae-a2e43d901e4d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stories , stem= stori  lema= story\n",
            "leaves , stem= leav  lema= leaf\n",
            "horses , stem= hors  lema= horse\n",
            "does , stem= doe  lema= doe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En general, la lematización es un proceso más robusto.\n",
        "\n",
        "Sin embargo, en el ejemplo anterior, la lematización no es capaz de obtener el lema para la forma verbal **does**.\n",
        "\n",
        "En NLTK, es posible pasar un segundo parámetro al objeto WordNetLemmatizer, para indicar la categoría morfosintáctica de la palabra que queremos lematizar ('n' para nombres,  'a' para adjetivos,  'v' para verbos, y 'r' para adverbios). Esto permite al lematizador manejar las palabras que pueden tener varias categorías morfosintácticas (por ejemplo, **leaves** puede ser un nombre plural, y la tercera personal del singular del verbo **leave**).\n"
      ],
      "metadata": {
        "id": "QUZsYP0-xa4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos más ejemplos donde la lematización es capaz de obtener el lema correcto, pero el stemming no es capaz de obtener una raíz correcta.\n"
      ],
      "metadata": {
        "id": "F8co19jNyUYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El lema de 'better' es 'good'. El stemming no es capaz de obtener esta relación.\n"
      ],
      "metadata": {
        "id": "iK7l-gKuzpbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word='better'\n",
        "print(word, \",  stem:\", stemmer.stem(word))\n",
        "print(word, \",  lema:\", lematizer.lemmatize(word, 'a'))\n"
      ],
      "metadata": {
        "id": "g1jJ06_-ycfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El gerundio 'meeting' tiene como lema 'meet', que va a coincidir con su stem. Sin embargo, el lema del sustantivo 'meeting' es 'meeting', que no coincide con el stem."
      ],
      "metadata": {
        "id": "uxdQ35nUzzrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word='meeting'\n",
        "print(word, \", its stem is:\", stemmer.stem(word))\n",
        "print(word, \", its lemma is:\", lematizer.lemmatize(word, 'v'))\n",
        "print(word, \", its lemma is:\", lematizer.lemmatize(word, 'n'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuPDXZ-rzz8s",
        "outputId": "00dccde7-56ac-459a-eb57-8165b3ef6f52"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "meeting , its stem is: meet\n",
            "meeting , its lemma is: meet\n",
            "meeting , its lemma is: meeting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word='drove'\n",
        "print(word, \" stem:\", stemmer.stem(word))\n",
        "print(word, \" lema:\", lematizer.lemmatize(word, 'v'))\n"
      ],
      "metadata": {
        "id": "GMS-TwCdykba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otro error típico en stemming  es que para algunos conjuntos de palabras (por ejemplo, las palabras **easy**, **easily**, **easier**, **easiest**), el algoritmo debería devolver la misma raíz (**easy**), pero no es así.\n",
        "La lematización sí es capaz de resolver correctamente el lema de esos adjetivos."
      ],
      "metadata": {
        "id": "S1o-4C8D0NBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in ['easy', 'easily', 'easier', 'easiest']:\n",
        "    print(word, ' stem=', stemmer.stem(word), \" lema:\", lematizer.lemmatize(word, 'a'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKfD4Z_e0Qjd",
        "outputId": "a50da015-458b-4a33-e18c-9f4ab1b4c28d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "easy  stem= easi  lema: easy\n",
            "easily  stem= easili  lema: easily\n",
            "easier  stem= easier  lema: easy\n",
            "easiest  stem= easiest  lema: easy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Stemming en otros idiomas\n",
        "\n",
        "\n",
        "Utiliza la clase **SnowballStemmer** (una versión mejorada de Porter):"
      ],
      "metadata": {
        "id": "OhMo9tIz02PL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from  nltk.stem import SnowballStemmer\n",
        "sno = SnowballStemmer('spanish')\n",
        "for word in ['cantar', 'cantaré', 'cantaba', 'cantó', 'cantaron']:\n",
        "    print(word, 'stem=', sno.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0SPpvso08Mb",
        "outputId": "d2f46765-5864-4b20-d98f-ae5b97c83bd4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cantar stem= cant\n",
            "cantaré stem= cant\n",
            "cantaba stem= cant\n",
            "cantó stem= cant\n",
            "cantaron stem= cant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por tanto, podemos concluir que la lematización es más robusta (aunque el stemming es más eficiente, más rápido).\n",
        "\n",
        "La lematización y stemming son tareas que nos permiten normalizar el texto, reduciendo la variabilidad léxica del lenguaje natural. Son muy útiles en algunas aplicaciones de PLN, como la clasificación de textos y la recuperación de información.\n",
        "Al normalizar los textos (lematizar o realizar stemming), estamos eliminando ruido, reduciendo el vocabulario asociado a una colección de textos, y por tanto, disminuyendo la dimensionalidad en los modelos de representación de bolsa de palabras o tf-idf.\n"
      ],
      "metadata": {
        "id": "weJgCpvVqKHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "-"
      ],
      "metadata": {
        "id": "Og6nAju4sy6p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPJrRhQskmK4"
      },
      "source": [
        "### Cómo eliminar las palabras vacías (Stopwords)\n",
        "Son palabras sin significado semántico: artículos, preposiciones, conjunciones, y algunos adverbios.\n",
        "También palabras muy comunes como los verbos modales o verbos frecuentes en el idioma ('to be', 'to have', etc).\n",
        "\n",
        "Es útil identificar estas palabras en los textos para poder ignorarlas durante la representación de los textos, ya que no añaden información semántica y pueden añadir ruido.\n",
        "\n",
        "\n",
        "Es necesario descargar el paquete **stopwords**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seujp-10kpLR",
        "outputId": "0923d905-f82f-48e1-8b3f-c94f8586906b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK tiene una lista predefinida de palabras vacías que hace referencia a las palabras más comunes.\n"
      ],
      "metadata": {
        "id": "zhe_Wrb61P_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toZdlX9YvLgN",
        "outputId": "5453c39d-c076-4c10-d651-88e1326f6787"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SlxKpeLkz3H"
      },
      "source": [
        "En la siguiente celda, vemos un ejemplo de código para eliminar las stopwords del texto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP5Bmp_Kkugn",
        "outputId": "b9a1e697-e5b3-4c32-d25e-02a03d5bed14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "stop_words = sorted(stopwords.words(\"spanish\"))\n",
        "\n",
        "text = 'El ajedrez es un juego de estrategia.'\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "relevant_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "print(relevant_tokens)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['El', 'ajedrez', 'es', 'un', 'juego', 'de', 'estrategia', '.']\n",
            "['ajedrez', 'juego', 'estrategia', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGxts6eqs9e-"
      },
      "source": [
        "### Cómo obtener  entidades nombradas en un texto\n",
        "\n",
        "\n",
        "NLTK proporciona un método **ne_chunks**, capaz de reconocer entidades del tipo PERSONA, ORGANIZACIÓN y GEP.\n",
        "\n",
        "Descarga los siguientes paquetes:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdmesUb4swU3",
        "outputId": "3da24410-d892-4a88-bd34-f0d20e46eaae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero vamos a ver cómo podemos obtener los sintagmas nominales de un texto:"
      ],
      "metadata": {
        "id": "IC2ekC6622rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"At least four people were dead after a man began shooting at a synagogue in the Squirrel Hill neighbourhood of Pittsburgh on Saturday.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "tags=nltk.pos_tag(tokens)\n",
        "ner_tags = nltk.ne_chunk(tags)\n",
        "print(ner_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vixQ07vLyI2A",
        "outputId": "844615c9-273d-47b6-a3ba-29439e4002e9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  At/IN\n",
            "  least/JJS\n",
            "  four/CD\n",
            "  people/NNS\n",
            "  were/VBD\n",
            "  dead/JJ\n",
            "  after/IN\n",
            "  a/DT\n",
            "  man/NN\n",
            "  began/VBD\n",
            "  shooting/VBG\n",
            "  at/IN\n",
            "  a/DT\n",
            "  synagogue/NN\n",
            "  in/IN\n",
            "  the/DT\n",
            "  (ORGANIZATION Squirrel/NNP Hill/NNP)\n",
            "  neighbourhood/NN\n",
            "  of/IN\n",
            "  (GPE Pittsburgh/NNP)\n",
            "  on/IN\n",
            "  Saturday/NNP\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El reconocimiento de entidades de NLTK no es muy robusto. Por ejemplo, en la siguiente celda, podemos ver cómo Apple es reconocida como 'PERSON'!!!"
      ],
      "metadata": {
        "id": "F6cwP0l84Oqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Apple opened its first retail store in Mumbai on Tuesday.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "tags=nltk.pos_tag(tokens)\n",
        "ner_tags = nltk.ne_chunk(tags)\n",
        "print(ner_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoT5RzwR3yAV",
        "outputId": "96789b17-2076-48ac-d27b-e9e60be84887"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Apple/NNP)\n",
            "  opened/VBD\n",
            "  its/PRP$\n",
            "  first/JJ\n",
            "  retail/JJ\n",
            "  store/NN\n",
            "  in/IN\n",
            "  (GPE Mumbai/NNP)\n",
            "  on/IN\n",
            "  Tuesday/NNP\n",
            "  ./.)\n"
          ]
        }
      ]
    }
  ]
}
